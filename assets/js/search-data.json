{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://emilearthur.github.io/fastblog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://emilearthur.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpy.random as nr import math %matplotlib inline %matplotlib widget . customer_info = pd.read_csv(&#39;Data/AdvWorksCusts.csv&#39;) customer_spending = pd.read_csv(&#39;Data/AW_AveMonthSpend.csv&#39;) customer_has_bike = pd.read_csv(&#39;Data/AW_BikeBuyer.csv&#39;) . (customer_info.astype(np.object) == &#39;?&#39;).any() (customer_spending.astype(np.object) == &#39;?&#39;).any() (customer_has_bike.astype(np.object) == &#39;?&#39;).any() # checking for missing values print((customer_info.astype(np.object).isnull()).any()) print((customer_spending.astype(np.object).isnull()).any()) print((customer_has_bike.astype(np.object).isnull()).any()) . print( &quot;Customer data&quot;) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) print(&#39; n&#39; + &quot;Customer Spending &quot;) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) print(&#39; n&#39; + &quot;Customer has bikes&quot;) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) . customer_info.drop_duplicates(subset=&#39;CustomerID&#39;, keep=&#39;last&#39;,inplace=True) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) . customer_spending.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) . customer_has_bike.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) . customer_info.describe().round() . customer_has_bike.describe().round() . #normalize to retun the relative frequency print(customer_has_bike.BikeBuyer.value_counts(normalize=True)) print(customer_has_bike.BikeBuyer.value_counts()) . combined = customer_info.merge(customer_has_bike, on = &#39;CustomerID&#39;, how=&#39;left&#39;) combined.head(5) . Running various visualization to see features to select for ML model def plot_box(combined, cols, col_x= &#39;BikeBuyer&#39;): for col in cols: sns.set_style(&quot;whitegrid&quot;) sns.boxplot(col_x, col, data=combined) plt.xlabel(col_x) # set x-axis plt.ylabel(col) # set y-axis plt.show() cols =[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39; ,&#39;NumberChildrenAtHome&#39;,&#39;TotalChildren&#39;] plot_box(combined, cols) . # forming categorical variables is_bike_buyer = combined.BikeBuyer== 1 bike_buyers = combined[is_bike_buyer] is_non_bike_buyer = combined.BikeBuyer == 0 non_bike_buyers = combined[is_non_bike_buyer] print(bike_buyers.shape) print(non_bike_buyers.shape) . # plot bar plot bike buyers counts def plot_bar(cat_cols): combined[&#39;dummy&#39;] = np.ones(shape = combined.shape[0]) for col in cat_cols: counts = combined[[&#39;dummy&#39;,&#39;BikeBuyer&#39;, col]].groupby([&#39;BikeBuyer&#39;,col], as_index = False).count() temp = counts[counts[&#39;BikeBuyer&#39;] ==0][[col,&#39;dummy&#39;]] temp.plot.bar(x=col, y= &#39;dummy&#39;) plt.title(&#39;Counts for &#39; + col + &#39; n non bike buyer&#39;) plt.ylabel(&#39;count&#39;) temp = counts[counts[&#39;BikeBuyer&#39;] == 1][[col,&#39;dummy&#39;]] temp.plot.bar(x=col, y=&#39;dummy&#39;) plt.title(&#39;Counts for &#39; + col + &#39; n bike buyer&#39;) plt.ylabel(&#39;count&#39;) plt.show() . cols = [&#39;Occupation&#39;,&#39;Gender&#39;,&#39;MaritalStatus&#39;] plot_bar(cols) . has_child_at_home = [] def generate_has_child_at_home(customer_info, has_child_at_home): for index, row in customer_info.iterrows(): if row.NumberChildrenAtHome&gt;0: has_child_at_home.append(&#39;Y&#39;) else: has_child_at_home.append(&#39;N&#39;) return has_child_at_home combined[&#39;hasChildAtHome&#39;] = generate_has_child_at_home(customer_info, has_child_at_home) combined[[&#39;hasChildAtHome&#39;,&#39;NumberChildrenAtHome&#39;]].head() . from datetime import datetime from dateutil.parser import parse def generate_age(data, format): collect_date = birthday = datetime(1998,1,1,0,0,0) age = [] for index, row in data.iterrows(): cust_date = datetime.strptime(row[&#39;BirthDate&#39;], format) age.append(int((collect_date - cust_date).days/365)) return age . data[&#39;Age&#39;] = generate_age(data, &#39;%Y-%m-%d&#39;) data[[&#39;BirthDate&#39;,&#39;Age&#39;]].head() . cols = [&#39;hasChildAtHome&#39;] plot_bar(cols) . features_chosen = [&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;,&#39;Occupation&#39;,&#39;Gender&#39;,&#39;MaritalStatus&#39;, &#39;hasChildAtHome&#39;] features = combined[features_chosen] features.head() . Preparing data for scikit learn . 1. encode categorical variable using one hot encoding. 2. convert features and labels to numpy arrays. . from sklearn import preprocessing import sklearn.model_selection as ms from sklearn import linear_model import sklearn.metrics as sklm . labels = np.array(combined.BikeBuyer) print(labels) . def encode_string(cat_features): enc = preprocessing.LabelEncoder() enc.fit(cat_features) enc_cat_features = enc.transform(cat_features) ohe = preprocessing.OneHotEncoder() encoded = ohe.fit(enc_cat_features.reshape(-1,1)) return encoded.transform(enc_cat_features.reshape(-1,1)).toarray() . def encode_cat_features(features): cat_features = [&#39;Gender&#39;,&#39;MaritalStatus&#39;,&#39;hasChildAtHome&#39;] f = encode_string(features[&#39;Occupation&#39;]) for cat in cat_features: enc = encode_string(features[cat]) f = np.concatenate([f, enc], 1) return f . numeric_features = np.array(combined[[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;]]) . encoded_features = encode_cat_features(features) . features = np.concatenate([encoded_features,numeric_features],1) features.shape . features[3,:13] . nr.seed(9988) indx = range(features.shape[0]) indx = ms.train_test_split(indx, test_size=300) X_train = features[indx[0],:] y_train = np.ravel(labels[indx[0]]) X_test = features[indx[1],:] y_test = np.ravel(labels[indx[1]]) . X_train[:2] . scalar = preprocessing.MinMaxScaler(feature_range=(-1,1)).fit(X_train[:,11:]) X_train[:,11:] = scalar.transform(X_train[:,11:]) X_test[:,11:] = scalar.transform(X_test[:,11:]) . X_train[:2] . # Due to class inbalanc for bike buyers and no bike buyer, the class weight parameter is used logistic_mod = linear_model.LogisticRegression(class_weight=&#39;balanced&#39;) . nr.seed(123) inside = ms.KFold(n_splits=10, shuffle=True) nr.seed(321) outside = ms.KFold(n_splits=10, shuffle=True) nr.seed(3456) param_grid = {&quot;C&quot;: [0.1,1,10,100,1000]} clf = ms.GridSearchCV(estimator=logistic_mod, param_grid=param_grid, cv=inside, # using the inside folds scoring = &#39;roc_auc&#39;, return_train_score = True) clf.fit(features,labels) clf.best_estimator_.C . nr.seed(498) cv_estimate = ms.cross_val_score(clf, features, labels, cv = outside) # use the outside folds print(&#39;Mean perfomance metic = %4.3f&#39; %np.mean(cv_estimate)) print(&#39;STD of the metric = %4.3f&#39; %np.std(cv_estimate)) print(&#39;Outcome by cv fold&#39;) for i, x in enumerate (cv_estimate): print(&#39;Fold %2d %4.3f&#39; % (i+1,x)) . logistic_mod = linear_model.LogisticRegression(C=clf.best_estimator_.C, class_weight=&#39;balanced&#39;) logistic_mod.fit(X_train,y_train) print(logistic_mod.intercept_) print(logistic_mod.coef_) . probabilities = logistic_mod.predict_proba(X_test) print(probabilities[:15,:]) . def score_model(probs, threshold): return np.array([1 if x &gt; threshold else 0 for x in probs[:,1]]) threshold = 0.51 scores = score_model(probabilities, threshold) print(np.array(scores[:18])) print(y_test[:18]) . def print_matrics(labels, scores): metrics = sklm.precision_recall_fscore_support(labels, scores) conf = sklm.confusion_matrix(labels, scores) print(&#39; Confusion matrix&#39;) print(&#39; Score positive Score negative&#39;) print(&#39;Actual positive %6d&#39; % conf[0,0] + &#39; %5d&#39; % conf[0,1]) print(&#39;Actual negative %6d&#39; % conf[1,0] + &#39; %5d&#39; % conf[1,1]) print(&#39;&#39;) print(&#39;Accuracy %0.2f&#39; % sklm.accuracy_score(labels, scores)) print(&#39; &#39;) print(&#39; Positive Negative&#39;) print(&#39;Num case %6d&#39; % metrics[3][0] + &#39; %6d&#39; % metrics[3][1]) print(&#39;Precision %6.2f&#39; % metrics[0][0] + &#39; %6.2f&#39; % metrics[0][1]) print(&#39;Recall %6.2f&#39; % metrics[1][0] + &#39; %6.2f&#39; % metrics[1][1]) print(&#39;F1 %6.2f&#39; % metrics[2][0] + &#39; %6.2f&#39; % metrics[2][1]) . print_matrics(y_test, scores) . def plot_auc(labels, probs): ## compute the false postive rate, true positive rate and threshold along with the AUC fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1]) auc = sklm.auc(fpr, tpr) ## plot the result plt.title(&#39;Reciever Operating Charateristic&#39;) plt.plot(fpr, tpr, color = &#39;orange&#39;, label = &#39;AUC = %0.2f&#39; %auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([0,1]) plt.ylim([0,1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() plot_auc(y_test, probabilities) . from sklearn.ensemble import RandomForestClassifier param_grid = {&#39;max_features&#39;: [2,3,5,10,13], &#39;min_samples_leaf&#39;:[3,5,10,20]} nr.seed(3456) rf_clf = RandomForestClassifier(class_weight = &#39;balanced&#39;) nr.seed(4455) rf_clf = ms.GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv = inside, # Use the inside folds scoring = &#39;roc_auc&#39;, return_train_score = True) rf_clf.fit(features, labels) print(rf_clf.best_estimator_.max_features) print(rf_clf.best_estimator_.min_samples_leaf) . nr.seed(1115) rf_mod = RandomForestClassifier(class_weight=&#39;balanced&#39;, max_features = rf_clf.best_estimator_.max_features, min_samples_leaf =rf_clf.best_estimator_.min_samples_leaf) rf_mod.fit(X_train,y_train) probabilities = rf_mod.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test,probabilities) . #nr.seed(1115) from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;linear&#39;,probability=True, random_state= 0) svclassifier.fit(X_train,y_train) probabilities = svclassifier.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test, probabilities) . from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;linear&#39;, random_state=0) svclassifier.fit(X_train,y_train) param_grid = [{&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;linear&#39;]}, {&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}] svm_clf = ms.GridSearchCV(estimator= svclassifier, param_grid= param_grid, scoring= &#39;accuracy&#39;, cv= 3, n_jobs=-1) svm_clf.fit(X_train,y_train) . print(svm_clf.best_score_) print(svm_clf.best_params_) print(svm_clf.best_estimator_) . from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;rbf&#39;,C=1,probability=True, gamma=0.7) svclassifier.fit(X_train,y_train) probabilities = svclassifier.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test, probabilities) . Looking at the accuracy of all the models used seems we settle on SVM with the parameters used. . final = pd.read_csv(&#39;Data/AW_test.csv&#39;) final[&#39;hasChildAtHome&#39;] = generate_has_child_at_home(final,[]) final_features = final[features_chosen] numeric_final_features = np.array(final_features[[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;]]) encoded_final_features = encode_cat_features(final_features) final_features = np.concatenate([encoded_final_features, numeric_final_features],1) final_features[:,11:] = scalar.transform(final_features[:,11:]) . probabilities = svclassifier.predict_proba(final_features) scores = score_model(probabilities, 0.54) . print(scores) . np.savetxt(&#39;final_answer_classification.csv&#39;,scores,delimiter=&#39;,&#39;,fmt=&#39;%i&#39;) .",
            "url": "https://emilearthur.github.io/fastblog/2019/08/31/classificationbikebuyers.html",
            "relUrl": "/2019/08/31/classificationbikebuyers.html",
            "date": " • Aug 31, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi 👋, I&#39;m Emile Bondzie-Arthur . A Business and Systems Analyst at Fido MicroCredit Limited in Accra, Ghana. Love Data and working with Data. . 🌱 I’m currently learning PyTorch &amp; Deep Learning &amp; More python . | 👨‍💻 All of my projects are available at https://emilearthur.github.io/ . | . 📫 How to reach me frederickauthur@hotmail.com .",
          "url": "https://emilearthur.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://emilearthur.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}