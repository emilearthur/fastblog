{
  
    
        "post0": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://emilearthur.github.io/fastblog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "BikeBuyer Classification",
            "content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpy.random as nr import math %matplotlib inline %matplotlib widget . customer_info = pd.read_csv(&#39;Data/AdvWorksCusts.csv&#39;) customer_spending = pd.read_csv(&#39;Data/AW_AveMonthSpend.csv&#39;) customer_has_bike = pd.read_csv(&#39;Data/AW_BikeBuyer.csv&#39;) . (customer_info.astype(np.object) == &#39;?&#39;).any() (customer_spending.astype(np.object) == &#39;?&#39;).any() (customer_has_bike.astype(np.object) == &#39;?&#39;).any() # checking for missing values print((customer_info.astype(np.object).isnull()).any()) print((customer_spending.astype(np.object).isnull()).any()) print((customer_has_bike.astype(np.object).isnull()).any()) . print( &quot;Customer data&quot;) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) print(&#39; n&#39; + &quot;Customer Spending &quot;) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) print(&#39; n&#39; + &quot;Customer has bikes&quot;) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) . customer_info.drop_duplicates(subset=&#39;CustomerID&#39;, keep=&#39;last&#39;,inplace=True) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) . customer_spending.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) . customer_has_bike.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) . customer_info.describe().round() . customer_has_bike.describe().round() . #normalize to retun the relative frequency print(customer_has_bike.BikeBuyer.value_counts(normalize=True)) print(customer_has_bike.BikeBuyer.value_counts()) . combined = customer_info.merge(customer_has_bike, on = &#39;CustomerID&#39;, how=&#39;left&#39;) combined.head(5) . Running various visualization to see features to select for ML model def plot_box(combined, cols, col_x= &#39;BikeBuyer&#39;): for col in cols: sns.set_style(&quot;whitegrid&quot;) sns.boxplot(col_x, col, data=combined) plt.xlabel(col_x) # set x-axis plt.ylabel(col) # set y-axis plt.show() cols =[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39; ,&#39;NumberChildrenAtHome&#39;,&#39;TotalChildren&#39;] plot_box(combined, cols) . # forming categorical variables is_bike_buyer = combined.BikeBuyer== 1 bike_buyers = combined[is_bike_buyer] is_non_bike_buyer = combined.BikeBuyer == 0 non_bike_buyers = combined[is_non_bike_buyer] print(bike_buyers.shape) print(non_bike_buyers.shape) . # plot bar plot bike buyers counts def plot_bar(cat_cols): combined[&#39;dummy&#39;] = np.ones(shape = combined.shape[0]) for col in cat_cols: counts = combined[[&#39;dummy&#39;,&#39;BikeBuyer&#39;, col]].groupby([&#39;BikeBuyer&#39;,col], as_index = False).count() temp = counts[counts[&#39;BikeBuyer&#39;] ==0][[col,&#39;dummy&#39;]] temp.plot.bar(x=col, y= &#39;dummy&#39;) plt.title(&#39;Counts for &#39; + col + &#39; n non bike buyer&#39;) plt.ylabel(&#39;count&#39;) temp = counts[counts[&#39;BikeBuyer&#39;] == 1][[col,&#39;dummy&#39;]] temp.plot.bar(x=col, y=&#39;dummy&#39;) plt.title(&#39;Counts for &#39; + col + &#39; n bike buyer&#39;) plt.ylabel(&#39;count&#39;) plt.show() . cols = [&#39;Occupation&#39;,&#39;Gender&#39;,&#39;MaritalStatus&#39;] plot_bar(cols) . has_child_at_home = [] def generate_has_child_at_home(customer_info, has_child_at_home): for index, row in customer_info.iterrows(): if row.NumberChildrenAtHome&gt;0: has_child_at_home.append(&#39;Y&#39;) else: has_child_at_home.append(&#39;N&#39;) return has_child_at_home combined[&#39;hasChildAtHome&#39;] = generate_has_child_at_home(customer_info, has_child_at_home) combined[[&#39;hasChildAtHome&#39;,&#39;NumberChildrenAtHome&#39;]].head() . from datetime import datetime from dateutil.parser import parse def generate_age(data, format): collect_date = birthday = datetime(1998,1,1,0,0,0) age = [] for index, row in data.iterrows(): cust_date = datetime.strptime(row[&#39;BirthDate&#39;], format) age.append(int((collect_date - cust_date).days/365)) return age . data[&#39;Age&#39;] = generate_age(data, &#39;%Y-%m-%d&#39;) data[[&#39;BirthDate&#39;,&#39;Age&#39;]].head() . cols = [&#39;hasChildAtHome&#39;] plot_bar(cols) . features_chosen = [&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;,&#39;Occupation&#39;,&#39;Gender&#39;,&#39;MaritalStatus&#39;, &#39;hasChildAtHome&#39;] features = combined[features_chosen] features.head() . Preparing data for scikit learn . 1. encode categorical variable using one hot encoding. 2. convert features and labels to numpy arrays. . from sklearn import preprocessing import sklearn.model_selection as ms from sklearn import linear_model import sklearn.metrics as sklm . labels = np.array(combined.BikeBuyer) print(labels) . def encode_string(cat_features): enc = preprocessing.LabelEncoder() enc.fit(cat_features) enc_cat_features = enc.transform(cat_features) ohe = preprocessing.OneHotEncoder() encoded = ohe.fit(enc_cat_features.reshape(-1,1)) return encoded.transform(enc_cat_features.reshape(-1,1)).toarray() . def encode_cat_features(features): cat_features = [&#39;Gender&#39;,&#39;MaritalStatus&#39;,&#39;hasChildAtHome&#39;] f = encode_string(features[&#39;Occupation&#39;]) for cat in cat_features: enc = encode_string(features[cat]) f = np.concatenate([f, enc], 1) return f . numeric_features = np.array(combined[[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;]]) . encoded_features = encode_cat_features(features) . features = np.concatenate([encoded_features,numeric_features],1) features.shape . features[3,:13] . nr.seed(9988) indx = range(features.shape[0]) indx = ms.train_test_split(indx, test_size=300) X_train = features[indx[0],:] y_train = np.ravel(labels[indx[0]]) X_test = features[indx[1],:] y_test = np.ravel(labels[indx[1]]) . X_train[:2] . scalar = preprocessing.MinMaxScaler(feature_range=(-1,1)).fit(X_train[:,11:]) X_train[:,11:] = scalar.transform(X_train[:,11:]) X_test[:,11:] = scalar.transform(X_test[:,11:]) . X_train[:2] . # Due to class inbalanc for bike buyers and no bike buyer, the class weight parameter is used logistic_mod = linear_model.LogisticRegression(class_weight=&#39;balanced&#39;) . nr.seed(123) inside = ms.KFold(n_splits=10, shuffle=True) nr.seed(321) outside = ms.KFold(n_splits=10, shuffle=True) nr.seed(3456) param_grid = {&quot;C&quot;: [0.1,1,10,100,1000]} clf = ms.GridSearchCV(estimator=logistic_mod, param_grid=param_grid, cv=inside, # using the inside folds scoring = &#39;roc_auc&#39;, return_train_score = True) clf.fit(features,labels) clf.best_estimator_.C . nr.seed(498) cv_estimate = ms.cross_val_score(clf, features, labels, cv = outside) # use the outside folds print(&#39;Mean perfomance metic = %4.3f&#39; %np.mean(cv_estimate)) print(&#39;STD of the metric = %4.3f&#39; %np.std(cv_estimate)) print(&#39;Outcome by cv fold&#39;) for i, x in enumerate (cv_estimate): print(&#39;Fold %2d %4.3f&#39; % (i+1,x)) . logistic_mod = linear_model.LogisticRegression(C=clf.best_estimator_.C, class_weight=&#39;balanced&#39;) logistic_mod.fit(X_train,y_train) print(logistic_mod.intercept_) print(logistic_mod.coef_) . probabilities = logistic_mod.predict_proba(X_test) print(probabilities[:15,:]) . def score_model(probs, threshold): return np.array([1 if x &gt; threshold else 0 for x in probs[:,1]]) threshold = 0.51 scores = score_model(probabilities, threshold) print(np.array(scores[:18])) print(y_test[:18]) . def print_matrics(labels, scores): metrics = sklm.precision_recall_fscore_support(labels, scores) conf = sklm.confusion_matrix(labels, scores) print(&#39; Confusion matrix&#39;) print(&#39; Score positive Score negative&#39;) print(&#39;Actual positive %6d&#39; % conf[0,0] + &#39; %5d&#39; % conf[0,1]) print(&#39;Actual negative %6d&#39; % conf[1,0] + &#39; %5d&#39; % conf[1,1]) print(&#39;&#39;) print(&#39;Accuracy %0.2f&#39; % sklm.accuracy_score(labels, scores)) print(&#39; &#39;) print(&#39; Positive Negative&#39;) print(&#39;Num case %6d&#39; % metrics[3][0] + &#39; %6d&#39; % metrics[3][1]) print(&#39;Precision %6.2f&#39; % metrics[0][0] + &#39; %6.2f&#39; % metrics[0][1]) print(&#39;Recall %6.2f&#39; % metrics[1][0] + &#39; %6.2f&#39; % metrics[1][1]) print(&#39;F1 %6.2f&#39; % metrics[2][0] + &#39; %6.2f&#39; % metrics[2][1]) . print_matrics(y_test, scores) . def plot_auc(labels, probs): ## compute the false postive rate, true positive rate and threshold along with the AUC fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1]) auc = sklm.auc(fpr, tpr) ## plot the result plt.title(&#39;Reciever Operating Charateristic&#39;) plt.plot(fpr, tpr, color = &#39;orange&#39;, label = &#39;AUC = %0.2f&#39; %auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([0,1]) plt.ylim([0,1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.show() plot_auc(y_test, probabilities) . from sklearn.ensemble import RandomForestClassifier param_grid = {&#39;max_features&#39;: [2,3,5,10,13], &#39;min_samples_leaf&#39;:[3,5,10,20]} nr.seed(3456) rf_clf = RandomForestClassifier(class_weight = &#39;balanced&#39;) nr.seed(4455) rf_clf = ms.GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv = inside, # Use the inside folds scoring = &#39;roc_auc&#39;, return_train_score = True) rf_clf.fit(features, labels) print(rf_clf.best_estimator_.max_features) print(rf_clf.best_estimator_.min_samples_leaf) . nr.seed(1115) rf_mod = RandomForestClassifier(class_weight=&#39;balanced&#39;, max_features = rf_clf.best_estimator_.max_features, min_samples_leaf =rf_clf.best_estimator_.min_samples_leaf) rf_mod.fit(X_train,y_train) probabilities = rf_mod.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test,probabilities) . #nr.seed(1115) from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;linear&#39;,probability=True, random_state= 0) svclassifier.fit(X_train,y_train) probabilities = svclassifier.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test, probabilities) . from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;linear&#39;, random_state=0) svclassifier.fit(X_train,y_train) param_grid = [{&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;linear&#39;]}, {&#39;C&#39;: [1, 10, 100, 1000], &#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}] svm_clf = ms.GridSearchCV(estimator= svclassifier, param_grid= param_grid, scoring= &#39;accuracy&#39;, cv= 3, n_jobs=-1) svm_clf.fit(X_train,y_train) . print(svm_clf.best_score_) print(svm_clf.best_params_) print(svm_clf.best_estimator_) . from sklearn.svm import SVC svclassifier = SVC(kernel=&#39;rbf&#39;,C=1,probability=True, gamma=0.7) svclassifier.fit(X_train,y_train) probabilities = svclassifier.predict_proba(X_test) scores = score_model(probabilities,0.54) print(print_matrics(y_test, scores)) plot_auc(y_test, probabilities) . Looking at the accuracy of all the models used seems we settle on SVM with the parameters used. . final = pd.read_csv(&#39;Data/AW_test.csv&#39;) final[&#39;hasChildAtHome&#39;] = generate_has_child_at_home(final,[]) final_features = final[features_chosen] numeric_final_features = np.array(final_features[[&#39;YearlyIncome&#39;,&#39;NumberCarsOwned&#39;]]) encoded_final_features = encode_cat_features(final_features) final_features = np.concatenate([encoded_final_features, numeric_final_features],1) final_features[:,11:] = scalar.transform(final_features[:,11:]) . probabilities = svclassifier.predict_proba(final_features) scores = score_model(probabilities, 0.54) . print(scores) . np.savetxt(&#39;final_answer_classification.csv&#39;,scores,delimiter=&#39;,&#39;,fmt=&#39;%i&#39;) .",
            "url": "https://emilearthur.github.io/fastblog/2019/08/31/classificationbikebuyers.html",
            "relUrl": "/2019/08/31/classificationbikebuyers.html",
            "date": " • Aug 31, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "BikeBuyer Regression",
            "content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpy.random as nr import math %matplotlib inline . customer_info = pd.read_csv(&#39;Data/AdvWorksCusts.csv&#39;) customer_spending = pd.read_csv(&#39;Data/AW_AveMonthSpend.csv&#39;) customer_has_bike = pd.read_csv(&#39;Data/AW_BikeBuyer.csv&#39;) . print(&quot;For customer_info: &quot; + &quot; n&quot;) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) print(&quot; n&quot; + &quot;For customer_spending&quot; + &quot; n&quot;) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) print(&quot; n&quot; + &quot;For customer_has_bike&quot; + &quot; n&quot;) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) # dropping duplicate if they exist customer_info.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;, inplace=True) customer_spending.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) customer_has_bike.drop_duplicates(subset=&#39;CustomerID&#39;,keep=&#39;last&#39;,inplace=True) # checking if duplicate are dropped print(&quot;For customer_info: &quot; + &quot; n&quot;) print(customer_info.shape) print(customer_info.CustomerID.unique().shape) print(&quot; n&quot; + &quot;For customer_spending&quot; + &quot; n&quot;) print(customer_spending.shape) print(customer_spending.CustomerID.unique().shape) print(&quot; n&quot; + &quot;For customer_has_bike&quot; + &quot; n&quot;) print(customer_has_bike.shape) print(customer_has_bike.CustomerID.unique().shape) . For customer_info: (16519, 23) (16404,) For customer_spending (16519, 2) (16404,) For customer_has_bike (16519, 2) (16404,) For customer_info: (16404, 23) (16404,) For customer_spending (16404, 2) (16404,) For customer_has_bike (16404, 2) (16404,) . print((customer_info.astype(np.object).isnull()).any()) print((customer_spending.astype(np.object).isnull().any())) print((customer_has_bike.astype(np.object).isnull().any())) . CustomerID False Title True FirstName False MiddleName True LastName False Suffix True AddressLine1 False AddressLine2 True City False StateProvinceName False CountryRegionName False PostalCode False PhoneNumber False BirthDate False Education False Occupation False Gender False MaritalStatus False HomeOwnerFlag False NumberCarsOwned False NumberChildrenAtHome False TotalChildren False YearlyIncome False dtype: bool CustomerID False AveMonthSpend False dtype: bool CustomerID False BikeBuyer False dtype: bool . Doing some exploratory analysis once the data is cleaned . print(round(customer_info.describe(),2)) print(&quot; n&quot;) print(round(customer_spending.describe(),2)) print(&quot; n&quot;) print(round(customer_has_bike.describe(),2)) . CustomerID HomeOwnerFlag NumberCarsOwned NumberChildrenAtHome count 16404.00 16404.00 16404.00 16404.00 mean 20240.14 0.67 1.50 0.99 std 5340.37 0.47 1.14 1.51 min 11000.00 0.00 0.00 0.00 25% 15617.75 0.00 1.00 0.00 50% 20231.50 1.00 2.00 0.00 75% 24862.25 1.00 2.00 2.00 max 29482.00 1.00 4.00 5.00 TotalChildren YearlyIncome count 16404.00 16404.00 mean 2.00 78129.67 std 1.68 39728.38 min 0.00 9482.00 25% 0.00 47808.75 50% 2.00 76125.00 75% 3.00 105211.75 max 5.00 196511.00 CustomerID AveMonthSpend count 16404.00 16404.00 mean 20240.14 72.39 std 5340.37 27.27 min 11000.00 22.00 25% 15617.75 52.00 50% 20231.50 68.00 75% 24862.25 84.00 max 29482.00 176.00 CustomerID BikeBuyer count 16404.00 16404.00 mean 20240.14 0.33 std 5340.37 0.47 min 11000.00 0.00 25% 15617.75 0.00 50% 20231.50 0.00 75% 24862.25 1.00 max 29482.00 1.00 . data = customer_info.merge(customer_spending, on=&#39;CustomerID&#39;, how=&#39;left&#39;) data.head() . CustomerID Title FirstName MiddleName LastName Suffix AddressLine1 AddressLine2 City StateProvinceName ... Education Occupation Gender MaritalStatus HomeOwnerFlag NumberCarsOwned NumberChildrenAtHome TotalChildren YearlyIncome AveMonthSpend . 0 11000 | NaN | Jon | V | Yang | NaN | 3761 N. 14th St | NaN | Rockhampton | Queensland | ... | Bachelors | Professional | M | M | 1 | 0 | 0 | 2 | 137947 | 89 | . 1 11001 | NaN | Eugene | L | Huang | NaN | 2243 W St. | NaN | Seaford | Victoria | ... | Bachelors | Professional | M | S | 0 | 1 | 3 | 3 | 101141 | 117 | . 2 11002 | NaN | Ruben | NaN | Torres | NaN | 5844 Linden Land | NaN | Hobart | Tasmania | ... | Bachelors | Professional | M | M | 1 | 1 | 3 | 3 | 91945 | 123 | . 3 11003 | NaN | Christy | NaN | Zhu | NaN | 1825 Village Pl. | NaN | North Ryde | New South Wales | ... | Bachelors | Professional | F | S | 0 | 1 | 0 | 0 | 86688 | 50 | . 4 11004 | NaN | Elizabeth | NaN | Johnson | NaN | 7553 Harness Circle | NaN | Wollongong | New South Wales | ... | Bachelors | Professional | F | S | 1 | 4 | 5 | 5 | 92771 | 95 | . 5 rows × 24 columns . Below function is useful but I prefer you use the other . because its simple . from datetime import datetime from dateutil.parser import parse def generate_age(data, format): collect_date = birthday = datetime(1998,1,1,0,0,0) age = [] for index, row in data.iterrows(): cust_date = datetime.strptime(row[&#39;BirthDate&#39;], format) age.append(int((collect_date - cust_date).days/365)) return age . data[&#39;Age&#39;] = generate_age(data, &#39;%Y-%m-%d&#39;) data[[&#39;BirthDate&#39;,&#39;Age&#39;]].head() . BirthDate Age . 0 1966-04-08 | 31 | . 1 1965-05-14 | 32 | . 2 1965-08-12 | 32 | . 3 1968-02-15 | 29 | . 4 1968-08-08 | 29 | . generating age since we given the birthrate . This function for generating age work but it not safe since it does work with one form of format. . from datetime import datetime from datetime import date def calcute_age(age): cust_date = datetime.strptime(age, &quot;%Y-%m-%d&quot;) f_date = date(1998,1,1) return f_date.year - cust_date.year - ((f_date.month, f_date.day) &lt;(cust_date.month, cust_date.day)) data[&#39;Age&#39;] = data[&#39;BirthDate&#39;].apply(calcute_age) . data[[&#39;BirthDate&#39;,&#39;Age&#39;]].head() . def plot_scatter(auto_prices, cols, col_y= &#39;AveMonthSpend&#39;): for col in cols: fig = plt.figure(figsize=(7,6)) # define plot area ax = fig.gca() # define axis auto_prices.plot.scatter(x= col, y=col_y, ax= ax) ax.set_title(&#39;Scatter plot of &#39; + col_y + &#39; vs. &#39; + col) #title of the plot ax.set_xlabel(col) #set x axis text ax.set_ylabel(col_y) #set y axis text plt.show() . cols=[&#39;NumberChildrenAtHome&#39;,&#39;NumberCarsOwned&#39;,&#39;TotalChildren&#39;] plot_scatter(data,cols) . cols= [&#39;AveMonthSpend&#39;,&#39;YearlyIncome&#39;,&#39;Age&#39;] sns.pairplot(data[cols], palette=&quot;Set2&quot;, diag_kind=&quot;kde&quot;, size=2).map_upper(sns.kdeplot,cmap=&quot;Blues_d&quot;) . /usr/local/lib/python3.7/dist-packages/seaborn/axisgrid.py:2065: UserWarning: The `size` parameter has been renamed to `height`; pleaes update your code. warnings.warn(msg, UserWarning) /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . &lt;seaborn.axisgrid.PairGrid at 0x7f6b554f6c50&gt; . def plot_box(auto_prices, cols, col_y=&#39;AveMonthSpend&#39;): for col in cols: sns.set_style(&quot;whitegrid&quot;) sns.boxplot(col,col_y, data=auto_prices) plt.xlabel(col) #set x axis text plt.ylabel(col_y) #set y axis text plt.show() . cols= [&#39;Occupation&#39;,&#39;Gender&#39;,&#39;MaritalStatus&#39;,&#39;HomeOwnerFlag&#39;] plot_box(data, cols) . After visualizations above we selected the following features for model: Gender, MaritalStatus, HomeOwnerFlag, Occupation, Age, YearlyIncme and NumberChildrenAtHome . categorical_features= [&#39;Gender&#39;,&#39;MaritalStatus&#39;,&#39;HomeOwnerFlag&#39;,&#39;Occupation&#39;] numeric_features= [&#39;Age&#39;,&#39;YearlyIncome&#39;,&#39;NumberChildrenAtHome&#39;] . from sklearn import preprocessing import sklearn.model_selection as ms from sklearn import linear_model import sklearn.metrics as sklm def encode_string(cat_features): enc= preprocessing.LabelEncoder() enc.fit(cat_features) enc_cat_features= enc.transform(cat_features) ohe= preprocessing.OneHotEncoder() encoded= ohe.fit(enc_cat_features.reshape(-1,1)) return encoded.transform(enc_cat_features.reshape(-1,1)).toarray() . def encode_cat_features(features): categorical_features= [&#39;Gender&#39;,&#39;MaritalStatus&#39;,&#39;HomeOwnerFlag&#39;] f= encode_string(features[&#39;Occupation&#39;]) for cat in categorical_features: enc= encode_string(features[cat]) f= np.concatenate([f,enc],1) return f . labels = np.array(data.AveMonthSpend) selected = numeric_features + categorical_features features = data[selected] print(labels) print(features.head()) . [ 89 117 123 ... 79 65 68] Age YearlyIncome NumberChildrenAtHome Gender MaritalStatus 0 31 137947 0 M M 1 32 101141 3 M S 2 32 91945 3 M M 3 29 86688 0 F S 4 29 92771 5 F S HomeOwnerFlag Occupation 0 1 Professional 1 0 Professional 2 1 Professional 3 0 Professional 4 1 Professional . encoded_features= encode_cat_features(features) print(encoded_features[:,:]) #selecting numeric features and converting them to array numeric_features= np.array(data[numeric_features]) print(numeric_features[:,:]) . [[0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 1. 1. 0.] [0. 0. 0. ... 0. 0. 1.] ... [0. 1. 0. ... 0. 0. 1.] [0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 1. 0. 1.]] [[ 31 137947 0] [ 32 101141 3] [ 32 91945 3] ... [ 58 133053 0] [ 51 31930 0] [ 52 59382 0]] . /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) . features= np.concatenate([encoded_features,numeric_features],1) print(features.shape) print(features[:1,:]) . (16404, 14) [[0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 3.10000e+01 1.37947e+05 0.00000e+00]] . nr.seed(9988) indx= range(features.shape[0]) indx= ms.train_test_split(indx, test_size= 300) X_train= features[indx[0],:] y_train= np.ravel(labels[indx[0]]) X_test= features[indx[1],:] y_test= np.ravel(labels[indx[1]]) . # Scaling the data to avoid features having different magnitudes #scalar= preprocessing.MinMaxScaler(feature_range=(-1,1)).fit(X_train[:,11:]) scaler = preprocessing.StandardScaler().fit(X_train[:,11:13]) X_train[:,11:13] = scaler.transform(X_train[:,11:13]) X_test[:,11:13] = scaler.transform(X_test[:,11:13]) X_train[:2] . array([[ 1. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 1. , 1. , 0. , -0.04218537, -0.96803832, 0. ], [ 0. , 1. , 0. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , -0.48707941, 2.37320265, 0. ]]) . Now Features are prepared we try it on models . . lin_mod= linear_model.Ridge(alpha = 0.05) lin_mod.fit(X_train,y_train) print(lin_mod.intercept_) print(lin_mod.coef_) . 60.84370494780248 [ 6.62453133e-01 -2.17172381e+00 -1.79250561e-01 8.84114274e-01 8.04406961e-01 -1.40857718e+01 1.40857718e+01 3.09843371e+00 -3.09843371e+00 3.71975025e-03 -3.71975028e-03 -1.24172260e+00 8.20152838e+00 1.10719302e+01] . alphas = np.array([0.1,0.01,0.001,0.0001,0,0.01,0.05,0.04,0.03,0.02,1,2,3,4,5,6,7,8,9,10]) lin_mod= linear_model.Ridge() linRidge_clf = ms.GridSearchCV(estimator=lin_mod, param_grid=dict(alpha=alphas)) linRidge_clf.fit(X_train,y_train) #summarize results of grid search print(linRidge_clf.best_score_) print(linRidge_clf.best_estimator_.alpha) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for &#39;cv&#39; instead of relying on the default value. The default value will change from 3 to 5 in version 0.22. warnings.warn(CV_WARNING, FutureWarning) . 0.9459672317634047 3.0 . lin_mod= linear_model.Ridge(alpha = 3.0) lin_mod.fit(X_train,y_train) print(lin_mod.intercept_) print(lin_mod.coef_) . 60.84421760308023 [ 6.59621956e-01 -2.16533531e+00 -1.82820198e-01 8.85518088e-01 8.03015464e-01 -1.40810939e+01 1.40810939e+01 3.09746333e+00 -3.09746333e+00 3.24850901e-03 -3.24850901e-03 -1.24213121e+00 8.19896041e+00 1.10711523e+01] . def print_metrics(y_true, y_predicted): # compute R^2 and the adjusted R^2 r2= sklm.r2_score(y_true,y_predicted) n= X_test.shape[0] p= X_test.shape[1]-1 r2_adj= 1-(1-r2)*((n-1)/(n-p-1)) ## Print the usual metrics and the R^2 values print(&#39;Mean Square Error = &#39; + str(sklm.mean_squared_error(y_true, y_predicted))) print(&#39;Root Mean Square Error = &#39; + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted)))) print(&#39;Mean Absolute Error = &#39; + str(sklm.mean_absolute_error(y_true, y_predicted))) print(&#39;Median Absolute Error = &#39; + str(sklm.median_absolute_error(y_true, y_predicted))) print(&#39;R^2 = &#39; + str(r2)) print(&#39;Adjusted R^2 = &#39; + str(r2_adj)) . def print_evalute(y_true_, y_predicted_): errors= abs(y_predicted_ - y_true_) mape_= 100* np.mean(errors/y_true_) accuracy= 100 - mape_ print(&#39;Model Performance&#39;) print(&#39;Average Error: {:0.4f} degrees.&#39;.format(np.mean(errors))) print(&#39;Accuracy= {:0.2f}%.&#39;.format(accuracy)) . scores= lin_mod.predict(X_test) print_metrics(y_test, scores) print_evalute(y_test, scores) . Mean Square Error = 34.300889201469865 Root Mean Square Error = 5.856696099463405 Mean Absolute Error = 4.394083754414235 Median Absolute Error = 3.3884315387464348 R^2 = 0.9522663978047706 Adjusted R^2 = 0.9500966886140784 Model Performance Average Error: 4.3941 degrees. Accuracy= 93.28%. . def hist_residue(y_test, y_score): ## compute vector of residuals residue = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1)) # making a plot sns.distplot(residue) plt.title(&#39;Histogram of residuals&#39;) plt.xlabel(&#39;Residual value&#39;) plt.ylabel(&#39;Count&#39;) plt.show() hist_residue(y_test,scores) . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . def plot_residue(y_test, y_score): ## compute vector of residuals residue = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1)) # making a plot sns.regplot(y_score, residue, fit_reg= False) plt.title(&#39;Residuals vs Predicted values&#39;) plt.xlabel(&#39;Predicted Values&#39;) plt.ylabel(&#39;Residuals&#39;) plt.show() plot_residue(y_test,scores) . The residual are not normally distrubuted as expected. Also there is a pattern for lower Average residuals. This indicate the model is not generalize as expected. . from sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures poly_mod= make_pipeline(PolynomialFeatures(4), linear_model.LinearRegression()) poly_mod.fit(X_train,y_train) scores = poly_mod.predict(X_test) print_metrics(y_test,scores) print_evalute(y_test, scores) hist_residue(y_test,scores) plot_residue(y_test,scores) . Mean Square Error = 7.956054222480307 Root Mean Square Error = 2.8206478373735893 Mean Absolute Error = 2.2826290766398114 Median Absolute Error = 2.027923583984375 R^2 = 0.9889282424992272 Adjusted R^2 = 0.9884249807946466 Model Performance Average Error: 2.2826 degrees. Accuracy= 96.32%. . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . Comparing the polynomial feature to the linear regression. It can be seen that polynomial regression performs beter. The R2 and adj. R2 shows a good residual distrubution and also the histogram shows a form of a normal distribution. Due to this I will expore other model to see how best it goes. . # to choose the best parameter for the GradientBoostingRegressor from sklearn.ensemble import GradientBoostingRegressor gbrt_mod= GradientBoostingRegressor(random_state=0) param_grid= { &#39;n_estimators&#39;: [10,20,30,40,50,100,200,300,500], &#39;max_features&#39;: [&#39;auto&#39;], &#39;max_depth&#39;: [1,2,4,6,8,10], &#39;learning_rate&#39;: [0.1], &#39;subsample&#39;: [1] } gbrt_clf= ms.GridSearchCV(estimator=gbrt_mod, param_grid=param_grid, n_jobs=4, cv=5, scoring=&#39;neg_mean_squared_error&#39;) gbrt_clf.fit(X_train,y_train) print(gbrt_clf.best_score_) print(gbrt_clf.best_params_) . -9.746757215191346 {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 4, &#39;max_features&#39;: &#39;auto&#39;, &#39;n_estimators&#39;: 200, &#39;subsample&#39;: 1} . from sklearn.ensemble import GradientBoostingRegressor gbrt_mod= GradientBoostingRegressor(n_estimators=200, max_depth=4) gbrt_mod.fit(X_train,y_train) scores= gbrt_mod.predict(X_test) print_metrics(y_test,scores) print_evalute(y_test, scores) hist_residue(y_test,scores) plot_residue(y_test,scores) . Mean Square Error = 9.259138303535538 Root Mean Square Error = 3.042883222132512 Mean Absolute Error = 2.4327966433305996 Median Absolute Error = 2.119264615257041 R^2 = 0.9871148522752397 Adjusted R^2 = 0.986529163742296 Model Performance Average Error: 2.4328 degrees. Accuracy= 96.11%. . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . from sklearn.neural_network import MLPRegressor regressor_mod= MLPRegressor(hidden_layer_sizes= (100,), activation= &#39;tanh&#39;, learning_rate= &#39;adaptive&#39;, max_iter=1000, random_state=9, learning_rate_init=0.001) regressor_mod.fit(X_train, y_train) scores= regressor_mod.predict(X_test) print_metrics(y_test,scores) print_evalute(y_test, scores) hist_residue(y_test,scores) plot_residue(y_test,scores) . Mean Square Error = 8.100806569138946 Root Mean Square Error = 2.8461915903780874 Mean Absolute Error = 2.296191108436508 Median Absolute Error = 2.1082752324486655 R^2 = 0.9887268030877984 Adjusted R^2 = 0.9882143850463346 Model Performance Average Error: 2.2962 degrees. Accuracy= 96.32%. . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . from sklearn.ensemble import RandomForestRegressor rf_Regressor_mod= RandomForestRegressor(n_estimators=40) rf_Regressor_mod.fit(X_train, y_train) scores= rf_Regressor_mod.predict(X_test) print_metrics(y_test,scores) print_evalute(y_test, scores) hist_residue(y_test,scores) plot_residue(y_test,scores) . Mean Square Error = 11.499512500000002 Root Mean Square Error = 3.391093112847243 Mean Absolute Error = 2.744166666666666 Median Absolute Error = 2.400000000000002 R^2 = 0.9839971158797088 Adjusted R^2 = 0.9832697120560593 Model Performance Average Error: 2.7442 degrees. Accuracy= 95.66%. . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . # parameters n_estimators= [int(x) for x in np.linspace(10,500,10)]# # trees in random forest max_features= [&#39;auto&#39;,&#39;sqrt&#39;]# # features to consider at every split max_depth= [int(x) for x in np.linspace(10,100,10)]# # maximum number of levels in tree max_depth.append(None) min_samples_split= [2,5,10] # minimum # samples required at each split a node min_samples_leaf= [1,2,4] # minimum # of samples required at each leaf node bootstrap= [True, False] # Method of selecting sample for training each tree param_distributions= {&#39;n_estimators&#39;: n_estimators, &#39;max_features&#39;: max_features, &#39;max_depth&#39;: max_depth, &#39;min_samples_split&#39;: min_samples_split, &#39;min_samples_leaf&#39;: min_samples_leaf, &#39;bootstrap&#39;: bootstrap} rf_Regressor_mod= RandomForestRegressor() rf_Regressor_clf= ms.RandomizedSearchCV(estimator= rf_Regressor_mod, param_distributions= param_distributions, n_iter= 100, cv=3, random_state=42, n_jobs=-1) rf_Regressor_clf.fit(X_train,y_train) print(&#39; n&#39;) print(rf_Regressor_clf.best_score_) print(rf_Regressor_clf.best_params_) . /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning . 0.9860394267493952 {&#39;n_estimators&#39;: 445, &#39;min_samples_split&#39;: 2, &#39;min_samples_leaf&#39;: 1, &#39;max_features&#39;: &#39;auto&#39;, &#39;max_depth&#39;: 10, &#39;bootstrap&#39;: True} . from sklearn.ensemble import RandomForestRegressor rf_Regressor_mod= RandomForestRegressor(n_estimators= 227, min_samples_split= 5, min_samples_leaf= 1, max_features= &#39;auto&#39;, max_depth= 10, bootstrap= &#39;True&#39;) rf_Regressor_mod.fit(X_train, y_train) scores= rf_Regressor_mod.predict(X_test) print_metrics(y_test,scores) print_evalute(y_test, scores) hist_residue(y_test,scores) plot_residue(y_test,scores) . Mean Square Error = 9.571726313153945 Root Mean Square Error = 3.093820665965296 Mean Absolute Error = 2.4688942994223027 Median Absolute Error = 2.1969817511908794 R^2 = 0.9866798503831755 Adjusted R^2 = 0.9860743890369562 Model Performance Average Error: 2.4689 degrees. Accuracy= 96.04%. . /usr/lib/python3/dist-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval . From all the models, it could be seen that ML regressor does good in general compared to the other models. . # importing the final test data final= pd.read_csv(&#39;Data/AW_test.csv&#39;) # checking if there are duplicate print(final.shape) print(final.CustomerID.unique().shape) . (500, 23) (500,) . final[&#39;Age&#39;] = generate_age(final,&#39;%m/%d/%Y&#39;) final[[&#39;Age&#39;,&#39;BirthDate&#39;]].head() . Age BirthDate . 0 53 | 1/5/1945 | . 1 33 | 10/4/1964 | . 2 64 | 1/12/1934 | . 3 39 | 9/22/1958 | . 4 32 | 3/19/1965 | . encoded = encode_cat_features(final) numeric_final_features = np.array(final[[&#39;Age&#39;,&#39;YearlyIncome&#39;, &#39;NumberChildrenAtHome&#39;]]) final_test = np.concatenate([encoded,numeric_final_features], 1) final_test[:,11:13]= scaler.transform(final_test[:,11:13]) . /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values. If you want the future behaviour and silence this warning, you can specify &#34;categories=&#39;auto&#39;&#34;. In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly. warnings.warn(msg, FutureWarning) . final_scores= regressor_mod.predict(final_test) np.savetxt(&#39;final_answer_regression.csv&#39;, final_scores, delimiter=&#39;,&#39;,fmt=&#39;%i&#39;) .",
            "url": "https://emilearthur.github.io/fastblog/2019/08/31/Regression-pricing.html",
            "relUrl": "/2019/08/31/Regression-pricing.html",
            "date": " • Aug 31, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Mortgage Loan prediction based on mortgage application.",
            "content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpy.random as nr import math from sklearn.model_selection import train_test_split %matplotlib inline %matplotlib widget pd.set_option(&#39;display.max_columns&#39;, 50) __author__ = &quot;Frederick Emile Bondzie-Arthur&quot; __email__ = &quot;Frederickauthur@hotmail.com&quot; . DISCOVER . train_data= pd.read_csv(&quot;data/train_values.csv&quot;) train_data_label= pd.read_csv(&quot;data/train_labels.csv&quot;) final= pd.read_csv(&#39;data/test_values.csv&#39;) . train_data.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant . 0 0 | 3 | 1 | 1 | 1 | 70.0 | 3 | 18 | 37 | 246 | 2 | 5 | 1 | 24.0 | 6203.0 | 44.230 | 60588.0 | 50.933 | 716.0 | 2642.0 | 4536 | False | . 1 1 | 1 | 1 | 3 | 1 | 178.0 | 3 | 369 | 52 | 299 | 1 | 5 | 1 | 57.0 | 5774.0 | 15.905 | 54821.0 | 100.000 | 1622.0 | 2108.0 | 2458 | False | . 2 2 | 2 | 1 | 3 | 1 | 163.0 | 3 | 16 | 10 | 306 | 2 | 5 | 1 | 67.0 | 6094.0 | 61.270 | 67719.0 | 100.000 | 760.0 | 1048.0 | 5710 | False | . 3 3 | 1 | 1 | 1 | 1 | 155.0 | 1 | 305 | 47 | 180 | 2 | 5 | 1 | 105.0 | 6667.0 | 6.246 | 78439.0 | 100.000 | 2025.0 | 2299.0 | 5888 | True | . 4 4 | 1 | 1 | 1 | 1 | 305.0 | 3 | 24 | 37 | 20 | 2 | 3 | 2 | 71.0 | 6732.0 | 100.000 | 63075.0 | 82.200 | 1464.0 | 1847.0 | 289 | False | . train_data_label.head() . row_id accepted . 0 0 | 1 | . 1 1 | 0 | . 2 2 | 1 | . 3 3 | 1 | . 4 4 | 1 | . print(train_data.shape) print(train_data_label.shape) . (500000, 22) (500000, 2) . data= train_data.merge(train_data_label, on=&#39;row_id&#39;) data.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant accepted . 0 0 | 3 | 1 | 1 | 1 | 70.0 | 3 | 18 | 37 | 246 | 2 | 5 | 1 | 24.0 | 6203.0 | 44.230 | 60588.0 | 50.933 | 716.0 | 2642.0 | 4536 | False | 1 | . 1 1 | 1 | 1 | 3 | 1 | 178.0 | 3 | 369 | 52 | 299 | 1 | 5 | 1 | 57.0 | 5774.0 | 15.905 | 54821.0 | 100.000 | 1622.0 | 2108.0 | 2458 | False | 0 | . 2 2 | 2 | 1 | 3 | 1 | 163.0 | 3 | 16 | 10 | 306 | 2 | 5 | 1 | 67.0 | 6094.0 | 61.270 | 67719.0 | 100.000 | 760.0 | 1048.0 | 5710 | False | 1 | . 3 3 | 1 | 1 | 1 | 1 | 155.0 | 1 | 305 | 47 | 180 | 2 | 5 | 1 | 105.0 | 6667.0 | 6.246 | 78439.0 | 100.000 | 2025.0 | 2299.0 | 5888 | True | 1 | . 4 4 | 1 | 1 | 1 | 1 | 305.0 | 3 | 24 | 37 | 20 | 2 | 3 | 2 | 71.0 | 6732.0 | 100.000 | 63075.0 | 82.200 | 1464.0 | 1847.0 | 289 | False | 1 | . data.dtypes . row_id int64 loan_type int64 property_type int64 loan_purpose int64 occupancy int64 loan_amount float64 preapproval int64 msa_md int64 state_code int64 county_code int64 applicant_ethnicity int64 applicant_race int64 applicant_sex int64 applicant_income float64 population float64 minority_population_pct float64 ffiecmedian_family_income float64 tract_to_msa_md_income_pct float64 number_of_owner-occupied_units float64 number_of_1_to_4_family_units float64 lender int64 co_applicant bool accepted int64 dtype: object . print((data.astype(np.object) == &#39;?&#39;).any()) . row_id False loan_type False property_type False loan_purpose False occupancy False loan_amount False preapproval False msa_md False state_code False county_code False applicant_ethnicity False applicant_race False applicant_sex False applicant_income False population False minority_population_pct False ffiecmedian_family_income False tract_to_msa_md_income_pct False number_of_owner-occupied_units False number_of_1_to_4_family_units False lender False co_applicant False accepted False dtype: bool . print((data.astype(np.object).isnull()).any()) . row_id False loan_type False property_type False loan_purpose False occupancy False loan_amount False preapproval False msa_md False state_code False county_code False applicant_ethnicity False applicant_race False applicant_sex False applicant_income True population True minority_population_pct True ffiecmedian_family_income True tract_to_msa_md_income_pct True number_of_owner-occupied_units True number_of_1_to_4_family_units True lender False co_applicant False accepted False dtype: bool . (data.isnull().sum()/ data.row_id.unique().shape[0] * 100).round(2) . row_id 0.00 loan_type 0.00 property_type 0.00 loan_purpose 0.00 occupancy 0.00 loan_amount 0.00 preapproval 0.00 msa_md 0.00 state_code 0.00 county_code 0.00 applicant_ethnicity 0.00 applicant_race 0.00 applicant_sex 0.00 applicant_income 7.99 population 4.49 minority_population_pct 4.49 ffiecmedian_family_income 4.49 tract_to_msa_md_income_pct 4.50 number_of_owner-occupied_units 4.51 number_of_1_to_4_family_units 4.51 lender 0.00 co_applicant 0.00 accepted 0.00 dtype: float64 . (final.isnull().sum()/ final.row_id.unique().shape[0] * 100).round(2) . row_id 0.00 loan_type 0.00 property_type 0.00 loan_purpose 0.00 occupancy 0.00 loan_amount 0.00 preapproval 0.00 msa_md 0.00 state_code 0.00 county_code 0.00 applicant_ethnicity 0.00 applicant_race 0.00 applicant_sex 0.00 applicant_income 8.03 population 4.50 minority_population_pct 4.50 ffiecmedian_family_income 4.49 tract_to_msa_md_income_pct 4.50 number_of_owner-occupied_units 4.51 number_of_1_to_4_family_units 4.51 lender 0.00 co_applicant 0.00 dtype: float64 . data.isnull().sum() . row_id 0 loan_type 0 property_type 0 loan_purpose 0 occupancy 0 loan_amount 0 preapproval 0 msa_md 0 state_code 0 county_code 0 applicant_ethnicity 0 applicant_race 0 applicant_sex 0 applicant_income 39948 population 22465 minority_population_pct 22466 ffiecmedian_family_income 22440 tract_to_msa_md_income_pct 22514 number_of_owner-occupied_units 22565 number_of_1_to_4_family_units 22530 lender 0 co_applicant 0 accepted 0 dtype: int64 . final.isnull().sum() . row_id 0 loan_type 0 property_type 0 loan_purpose 0 occupancy 0 loan_amount 0 preapproval 0 msa_md 0 state_code 0 county_code 0 applicant_ethnicity 0 applicant_race 0 applicant_sex 0 applicant_income 40141 population 22480 minority_population_pct 22482 ffiecmedian_family_income 22453 tract_to_msa_md_income_pct 22517 number_of_owner-occupied_units 22574 number_of_1_to_4_family_units 22550 lender 0 co_applicant 0 dtype: int64 . filter1 = data[&quot;msa_md&quot;].isin([-1]) filter2 = data[&quot;county_code&quot;].isin([-1]) filter3 = data[&quot;state_code&quot;].isin([-1]) # displaying dataframe with all filter applied and mandatory data[filter1 | filter2| filter3].head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant accepted . 17 17 | 2 | 2 | 3 | 1 | 138.0 | 3 | -1 | 37 | 59 | 2 | 5 | 1 | NaN | 4193.0 | 14.996 | 57774.0 | 74.411 | 1247.0 | 1998.0 | 2566 | True | 1 | . 26 26 | 1 | 1 | 1 | 1 | 113.0 | 1 | -1 | -1 | -1 | 2 | 5 | 2 | 54.0 | NaN | NaN | NaN | NaN | NaN | NaN | 2839 | False | 0 | . 35 35 | 1 | 1 | 3 | 1 | 168.0 | 3 | -1 | 36 | 151 | 2 | 5 | 2 | 65.0 | 3195.0 | 20.700 | 47253.0 | 100.000 | 339.0 | 814.0 | 2597 | False | 1 | . 38 38 | 1 | 1 | 1 | 2 | 88.0 | 1 | -1 | -1 | -1 | 2 | 5 | 2 | 104.0 | NaN | NaN | NaN | NaN | NaN | NaN | 788 | True | 0 | . 45 45 | 1 | 2 | 1 | 1 | 106.0 | 3 | -1 | 42 | 136 | 3 | 1 | 1 | 48.0 | NaN | NaN | NaN | NaN | NaN | NaN | 2318 | True | 0 | . print(data.msa_md[filter1].count()) print(data.county_code[filter2].count()) print(data.state_code[filter3].count()) . 76982 20466 19132 . print(round((data.msa_md[filter1].count()/data.row_id.unique().shape[0] * 100),2)) print(round((data.county_code[filter2].count()/data.row_id.unique().shape[0] * 100),2)) print(round((data.state_code[filter3].count()/data.row_id.unique().shape[0] * 100),2)) . 15.4 4.09 3.83 . print(data.shape) print(data.row_id.unique().shape) . (500000, 23) (500000,) . data.describe().round(2) . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender accepted . count 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 460052.00 | 477535.00 | 477534.00 | 477560.00 | 477486.00 | 477435.00 | 477470.00 | 500000.00 | 500000.0 | . mean 249999.50 | 1.37 | 1.05 | 2.07 | 1.11 | 221.75 | 2.76 | 181.61 | 23.73 | 144.54 | 2.04 | 4.79 | 1.46 | 102.39 | 5416.83 | 31.62 | 69235.60 | 91.83 | 1427.72 | 1886.15 | 3720.12 | 0.5 | . std 144337.71 | 0.69 | 0.23 | 0.95 | 0.33 | 590.64 | 0.54 | 138.46 | 15.98 | 100.24 | 0.51 | 1.02 | 0.68 | 153.53 | 2728.14 | 26.33 | 14810.06 | 14.21 | 737.56 | 914.12 | 1838.31 | 0.5 | . min 0.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | -1.00 | -1.00 | -1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 14.00 | 0.53 | 17858.00 | 3.98 | 4.00 | 1.00 | 0.00 | 0.0 | . 25% 124999.75 | 1.00 | 1.00 | 1.00 | 1.00 | 93.00 | 3.00 | 25.00 | 6.00 | 57.00 | 2.00 | 5.00 | 1.00 | 47.00 | 3744.00 | 10.70 | 59731.00 | 88.07 | 944.00 | 1301.00 | 2442.00 | 0.0 | . 50% 249999.50 | 1.00 | 1.00 | 2.00 | 1.00 | 162.00 | 3.00 | 192.00 | 26.00 | 131.00 | 2.00 | 5.00 | 1.00 | 74.00 | 4975.00 | 22.90 | 67526.00 | 100.00 | 1327.00 | 1753.00 | 3731.00 | 1.0 | . 75% 374999.25 | 2.00 | 1.00 | 3.00 | 1.00 | 266.00 | 3.00 | 314.00 | 37.00 | 246.00 | 2.00 | 5.00 | 2.00 | 117.00 | 6467.00 | 46.02 | 75351.00 | 100.00 | 1780.00 | 2309.00 | 5436.00 | 1.0 | . max 499999.00 | 4.00 | 3.00 | 3.00 | 3.00 | 100878.00 | 3.00 | 408.00 | 52.00 | 324.00 | 4.00 | 7.00 | 4.00 | 10139.00 | 37097.00 | 100.00 | 125248.00 | 100.00 | 8771.00 | 13623.00 | 6508.00 | 1.0 | . Since the exist some missing data. We use the median of the data to fill the missing values. . data_median= data.median() data_median . row_id 249999.500 loan_type 1.000 property_type 1.000 loan_purpose 2.000 occupancy 1.000 loan_amount 162.000 preapproval 3.000 msa_md 192.000 state_code 26.000 county_code 131.000 applicant_ethnicity 2.000 applicant_race 5.000 applicant_sex 1.000 applicant_income 74.000 population 4975.000 minority_population_pct 22.901 ffiecmedian_family_income 67526.000 tract_to_msa_md_income_pct 100.000 number_of_owner-occupied_units 1327.000 number_of_1_to_4_family_units 1753.000 lender 3731.000 co_applicant 0.000 accepted 1.000 dtype: float64 . final_median= final.median() final_median . row_id 249999.500 loan_type 1.000 property_type 1.000 loan_purpose 2.000 occupancy 1.000 loan_amount 162.000 preapproval 3.000 msa_md 192.000 state_code 26.000 county_code 131.000 applicant_ethnicity 2.000 applicant_race 5.000 applicant_sex 1.000 applicant_income 74.000 population 4975.000 minority_population_pct 22.955 ffiecmedian_family_income 67514.000 tract_to_msa_md_income_pct 100.000 number_of_owner-occupied_units 1326.000 number_of_1_to_4_family_units 1753.000 lender 3713.000 co_applicant 0.000 dtype: float64 . data.fillna(data_median,inplace=True) data.shape . (500000, 23) . final.fillna(data_median,inplace=True) final.shape . (500000, 22) . (data.isnull().sum()/ data.row_id.unique().shape[0] * 100).round(2) . row_id 0.0 loan_type 0.0 property_type 0.0 loan_purpose 0.0 occupancy 0.0 loan_amount 0.0 preapproval 0.0 msa_md 0.0 state_code 0.0 county_code 0.0 applicant_ethnicity 0.0 applicant_race 0.0 applicant_sex 0.0 applicant_income 0.0 population 0.0 minority_population_pct 0.0 ffiecmedian_family_income 0.0 tract_to_msa_md_income_pct 0.0 number_of_owner-occupied_units 0.0 number_of_1_to_4_family_units 0.0 lender 0.0 co_applicant 0.0 accepted 0.0 dtype: float64 . (final.isnull().sum()/ final.row_id.unique().shape[0] * 100).round(2) . row_id 0.0 loan_type 0.0 property_type 0.0 loan_purpose 0.0 occupancy 0.0 loan_amount 0.0 preapproval 0.0 msa_md 0.0 state_code 0.0 county_code 0.0 applicant_ethnicity 0.0 applicant_race 0.0 applicant_sex 0.0 applicant_income 0.0 population 0.0 minority_population_pct 0.0 ffiecmedian_family_income 0.0 tract_to_msa_md_income_pct 0.0 number_of_owner-occupied_units 0.0 number_of_1_to_4_family_units 0.0 lender 0.0 co_applicant 0.0 dtype: float64 . data.shape . (500000, 23) . accepted_rate= data.accepted.value_counts()/data.shape[0] accepted_rate . 1 0.500228 0 0.499772 Name: accepted, dtype: float64 . data.describe().round(2) . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender accepted . count 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.00 | 500000.0 | . mean 249999.50 | 1.37 | 1.05 | 2.07 | 1.11 | 221.75 | 2.76 | 181.61 | 23.73 | 144.54 | 2.04 | 4.79 | 1.46 | 100.12 | 5396.98 | 31.23 | 69158.88 | 92.20 | 1423.17 | 1880.15 | 3720.12 | 0.5 | . std 144337.71 | 0.69 | 0.23 | 0.95 | 0.33 | 590.64 | 0.54 | 138.46 | 15.98 | 100.24 | 0.51 | 1.02 | 0.68 | 147.47 | 2667.72 | 25.80 | 14478.23 | 13.99 | 721.03 | 893.72 | 1838.31 | 0.5 | . min 0.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | -1.00 | -1.00 | -1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 14.00 | 0.53 | 17858.00 | 3.98 | 4.00 | 1.00 | 0.00 | 0.0 | . 25% 124999.75 | 1.00 | 1.00 | 1.00 | 1.00 | 93.00 | 3.00 | 25.00 | 6.00 | 57.00 | 2.00 | 5.00 | 1.00 | 49.00 | 3805.00 | 11.19 | 60071.00 | 89.14 | 963.00 | 1323.00 | 2442.00 | 0.0 | . 50% 249999.50 | 1.00 | 1.00 | 2.00 | 1.00 | 162.00 | 3.00 | 192.00 | 26.00 | 131.00 | 2.00 | 5.00 | 1.00 | 74.00 | 4975.00 | 22.90 | 67526.00 | 100.00 | 1327.00 | 1753.00 | 3731.00 | 1.0 | . 75% 374999.25 | 2.00 | 1.00 | 3.00 | 1.00 | 266.00 | 3.00 | 314.00 | 37.00 | 246.00 | 2.00 | 5.00 | 2.00 | 112.00 | 6379.00 | 44.49 | 74714.25 | 100.00 | 1754.00 | 2275.00 | 5436.00 | 1.0 | . max 499999.00 | 4.00 | 3.00 | 3.00 | 3.00 | 100878.00 | 3.00 | 408.00 | 52.00 | 324.00 | 4.00 | 7.00 | 4.00 | 10139.00 | 37097.00 | 100.00 | 125248.00 | 100.00 | 8771.00 | 13623.00 | 6508.00 | 1.0 | . accepted_Summary= data.groupby(&#39;accepted&#39;) accepted_Summary.mean() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant . accepted . 0 249936.842896 | 1.353433 | 1.066310 | 2.191667 | 1.102399 | 194.352733 | 2.755372 | 170.484961 | 22.279283 | 139.265501 | 2.031226 | 4.740073 | 1.488403 | 89.696290 | 5328.818777 | 33.201373 | 68142.082690 | 91.293288 | 1393.644762 | 1869.383779 | 3704.499764 | 0.350492 | . 1 250062.099986 | 1.379107 | 1.029007 | 1.942066 | 1.116775 | 249.128605 | 2.774063 | 192.718844 | 25.173245 | 149.813813 | 2.041225 | 4.833056 | 1.436369 | 110.536831 | 5465.083798 | 29.251766 | 70174.743021 | 93.106654 | 1452.674053 | 1890.901325 | 3735.728684 | 0.449567 | . corr=data.drop([&#39;row_id&#39;,&#39;county_code&#39;,&#39;state_code&#39;], axis=1).corr(method=&#39;spearman&#39;).round(2) fig= plt.figure(figsize=(20,10)) colormap = sns.diverging_palette(220, 10, as_cmap=True) sns.heatmap(corr, cmap=colormap, annot=True) plt.xticks(rotation=45) plt.xticks(range(len(corr.columns)), corr.columns) plt.yticks(range(len(corr.columns)), corr.columns) plt.title(&#39;Spearman Correlation Heatmap&#39;) plt.show() plt.savefig(&#39;image1.png&#39;) . &lt;Figure size 432x288 with 0 Axes&gt; . corr.style.background_gradient().set_precision(2) . loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant accepted . loan_type 1 | -0.07 | -0.11 | -0.18 | 0.06 | -0.14 | -0.01 | -0.06 | -0.03 | -0.05 | -0.17 | 0.05 | 0.05 | -0.06 | -0.04 | 0.03 | 0.05 | -0.03 | -0.03 | 0.02 | . property_type -0.07 | 1 | -0.13 | 0.04 | -0.18 | 0.05 | -0.1 | 0.07 | 0.04 | 0.05 | -0.16 | -0.01 | -0.03 | -0.11 | -0.06 | -0.01 | 0.03 | -0.05 | -0.02 | -0.1 | . loan_purpose -0.11 | -0.13 | 1 | 0 | 0.02 | 0.52 | 0.08 | 0.03 | 0.03 | 0.01 | 0.05 | -0 | 0.03 | 0.05 | -0.02 | -0.01 | -0.02 | 0.05 | 0.01 | -0.13 | . occupancy -0.18 | 0.04 | 0 | 1 | -0.03 | 0.04 | -0.01 | 0.09 | 0.06 | 0.05 | 0.14 | -0.06 | 0.04 | -0.03 | -0.08 | -0.1 | -0.03 | 0 | -0.01 | 0.02 | . loan_amount 0.06 | -0.18 | 0.02 | -0.03 | 1 | -0.07 | 0.1 | 0.04 | -0 | -0.1 | 0.54 | 0.06 | 0.05 | 0.31 | 0.25 | 0.04 | -0.06 | 0.03 | 0.16 | 0.17 | . preapproval -0.14 | 0.05 | 0.52 | 0.04 | -0.07 | 1 | 0.11 | 0.02 | 0.02 | 0.02 | 0.02 | -0.01 | 0.02 | 0 | -0.07 | -0.02 | -0.01 | -0 | 0.01 | -0.02 | . msa_md -0.01 | -0.1 | 0.08 | -0.01 | 0.1 | 0.11 | 1 | -0.02 | -0.02 | 0 | 0.09 | 0.06 | 0.12 | 0.31 | -0.05 | 0.03 | -0.06 | 0.02 | -0.01 | 0.09 | . applicant_ethnicity -0.06 | 0.07 | 0.03 | 0.09 | 0.04 | 0.02 | -0.02 | 1 | 0.41 | 0.34 | 0.08 | -0.04 | -0.16 | 0.07 | 0.06 | 0.03 | 0.02 | 0.01 | -0.12 | -0 | . applicant_race -0.03 | 0.04 | 0.03 | 0.06 | -0 | 0.02 | -0.02 | 0.41 | 1 | 0.28 | 0.03 | -0.02 | -0.16 | -0.04 | 0.04 | 0.02 | 0.02 | 0.01 | -0.08 | 0.02 | . applicant_sex -0.05 | 0.05 | 0.01 | 0.05 | -0.1 | 0.02 | 0 | 0.34 | 0.28 | 1 | -0.13 | -0.02 | 0.06 | 0.02 | -0.05 | -0.04 | -0.03 | 0.02 | -0.29 | -0.05 | . applicant_income -0.17 | -0.16 | 0.05 | 0.14 | 0.54 | 0.02 | 0.09 | 0.08 | 0.03 | -0.13 | 1 | 0.03 | -0.04 | 0.23 | 0.24 | 0.05 | -0.03 | 0.02 | 0.3 | 0.18 | . population 0.05 | -0.01 | -0 | -0.06 | 0.06 | -0.01 | 0.06 | -0.04 | -0.02 | -0.02 | 0.03 | 1 | 0.11 | 0.02 | 0.14 | 0.79 | 0.78 | 0 | 0.02 | 0.02 | . minority_population_pct 0.05 | -0.03 | 0.03 | 0.04 | 0.05 | 0.02 | 0.12 | -0.16 | -0.16 | 0.06 | -0.04 | 0.11 | 1 | 0.05 | -0.3 | -0.24 | -0.19 | 0.02 | -0.1 | -0.07 | . ffiecmedian_family_income -0.06 | -0.11 | 0.05 | -0.03 | 0.31 | 0 | 0.31 | 0.07 | -0.04 | 0.02 | 0.23 | 0.02 | 0.05 | 1 | -0.02 | 0 | -0.15 | 0.02 | 0.02 | 0.07 | . tract_to_msa_md_income_pct -0.04 | -0.06 | -0.02 | -0.08 | 0.25 | -0.07 | -0.05 | 0.06 | 0.04 | -0.05 | 0.24 | 0.14 | -0.3 | -0.02 | 1 | 0.36 | 0.18 | 0.01 | 0.09 | 0.06 | . number_of_owner-occupied_units 0.03 | -0.01 | -0.01 | -0.1 | 0.04 | -0.02 | 0.03 | 0.03 | 0.02 | -0.04 | 0.05 | 0.79 | -0.24 | 0 | 0.36 | 1 | 0.87 | -0 | 0.05 | 0.04 | . number_of_1_to_4_family_units 0.05 | 0.03 | -0.02 | -0.03 | -0.06 | -0.01 | -0.06 | 0.02 | 0.02 | -0.03 | -0.03 | 0.78 | -0.19 | -0.15 | 0.18 | 0.87 | 1 | -0.01 | 0.03 | 0 | . lender -0.03 | -0.05 | 0.05 | 0 | 0.03 | -0 | 0.02 | 0.01 | 0.01 | 0.02 | 0.02 | 0 | 0.02 | 0.02 | 0.01 | -0 | -0.01 | 1 | 0 | 0.01 | . co_applicant -0.03 | -0.02 | 0.01 | -0.01 | 0.16 | 0.01 | -0.01 | -0.12 | -0.08 | -0.29 | 0.3 | 0.02 | -0.1 | 0.02 | 0.09 | 0.05 | 0.03 | 0 | 1 | 0.1 | . accepted 0.02 | -0.1 | -0.13 | 0.02 | 0.17 | -0.02 | 0.09 | -0 | 0.02 | -0.05 | 0.18 | 0.02 | -0.07 | 0.07 | 0.06 | 0.04 | 0 | 0.01 | 0.1 | 1 | . corr_with_acc=data.drop([&#39;row_id&#39;,&#39;county_code&#39;,&#39;state_code&#39;], axis=1).corr(method=&#39;spearman&#39;)[&#39;accepted&#39;].sort_values(ascending=False) plt.figure(figsize=(14,6)) corr_with_acc.drop(&quot;accepted&quot;).plot.bar() plt.show() plt.savefig(&#39;image8.png&#39;) . &lt;Figure size 432x288 with 0 Axes&gt; . cat_vars=[&#39;loan_type&#39;,&#39;property_type&#39;,&#39;loan_purpose&#39;,&#39;occupancy&#39;,&#39;preapproval&#39;,&#39;applicant_sex&#39;,&#39;co_applicant&#39;, &#39;applicant_sex&#39;,&#39;applicant_race&#39;,&#39;applicant_ethnicity&#39;,&#39;msa_md&#39;,&#39;state_code&#39;,&#39;county_code&#39;] num_vars=[&#39;loan_amount&#39;,&#39;population&#39;,&#39;applicant_income&#39;,&#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;,&#39;number_of_owner-occupied_units&#39;,&#39;number_of_1_to_4_family_units&#39;] . def plot_voilin(combined, cols, col_x= &#39;accepted&#39;): fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(30, 10)) for col, subplot in zip(cols, ax.flatten()): sns.set_style(&quot;whitegrid&quot;) sns.violinplot(col_x, col, data=combined,ax=subplot) for label in subplot.get_xticklabels(): label.set_rotation(90) #voilin plot for numerical variable plot_voilin(data, num_vars) plt.savefig(&#39;image1.png&#39;) . def plot_box(combined, cols, col_x= &#39;accepted&#39;): fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(30, 10)) for col, subplot in zip(cols, ax.flatten()): sns.set_style(&quot;whitegrid&quot;) sns.boxplot(col_x, col, data=combined,ax=subplot) for label in subplot.get_xticklabels(): label.set_rotation(90) #voilin plot for numerical variable plot_box(data, num_vars) plt.savefig(&#39;image2.png&#39;) . def plot_den_hist(combined, cols, bins=10, hist= False): fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 25)) for col, subplot in zip(cols, ax.flatten()): sns.distplot(combined[col], bins= bins, rug=True, hist=hist, ax=subplot) for label in subplot.get_xticklabels(): label.set_rotation(0) #KDE plot for numerical variable, histogram not enabled plot_den_hist(data, num_vars) plt.savefig(&#39;image3.png&#39;) . plot_den_hist(data, num_vars, hist=True) plt.savefig(&#39;image3b.png&#39;) . def plot_bar(cat_cols): fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(30, 10)) for col, subplot in zip(cat_cols, ax.flatten()): sns.countplot(data[col],hue=data[&#39;accepted&#39;],ax=subplot) for label in subplot.get_xticklabels(): label.set_rotation(90) # plotting bar graph for categorical variables plot_bar(cat_vars) plt.savefig(&#39;image5.png&#39;) . data[num_vars].hist(bins=25, figsize=(20, 10), layout=(4, 4)); plt.savefig(&#39;image6.png&#39;) . From the graph, it can be seen that all data features are skwed except the ffiecmedian_family_income. To fix this issue we apply log to the data features skewed. . Skewness is the measure of symmetry of a distrubution. For a normal distubution skewness=0 and thus it is symmetrical. When data is skewed towards the right, then it is a postive skew otherwise is viceversa. . Skewness between 0 to $ pm$ 5= acceptable | Skewness between $ pm$ 0.5 to $ pm$ 1= a problem | skewness between $ pm$ 1 or more= utmost | data.skew(axis=0) . row_id 1.286588e-17 loan_type 1.864712e+00 property_type 5.196600e+00 loan_purpose -1.333652e-01 occupancy 2.871840e+00 loan_amount 7.655279e+01 preapproval -2.242003e+00 msa_md 1.353241e-02 state_code -5.974008e-02 county_code 2.309361e-01 applicant_ethnicity 5.802958e-01 applicant_race -1.583676e+00 applicant_sex 1.370674e+00 applicant_income 2.317498e+01 population 2.947782e+00 minority_population_pct 1.068839e+00 ffiecmedian_family_income 8.063549e-01 tract_to_msa_md_income_pct -2.035543e+00 number_of_owner-occupied_units 1.942059e+00 number_of_1_to_4_family_units 2.080321e+00 lender -2.196283e-01 co_applicant 4.080284e-01 accepted -9.120028e-04 dtype: float64 . import scipy.stats as ss def cramers_v(x, y): confusion_matrix = pd.crosstab(x,y) chi2 = ss.chi2_contingency(confusion_matrix)[0] n = confusion_matrix.sum().sum() phi2 = chi2/n r,k = confusion_matrix.shape phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1)) rcorr = r-((r-1)**2)/(n-1) kcorr = k-((k-1)**2)/(n-1) return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1))) # function to print categorical variables after running it through the cramers_v dunction def print_crammer_values(data, cat_features, cats_x= &#39;accepted&#39;): for cat_ in cat_features: print(cat_+ &quot;: &quot;+ str(cramers_v(data[cat_],data[cats_x]).round(2))) # calculating for correlation between categorical variable and target variable print_crammer_values(data, cat_vars) . loan_type: 0.02 property_type: 0.11 loan_purpose: 0.17 occupancy: 0.03 preapproval: 0.15 applicant_sex: 0.09 co_applicant: 0.1 applicant_sex: 0.09 applicant_race: 0.15 applicant_ethnicity: 0.11 msa_md: 0.17 state_code: 0.21 county_code: 0.2 . # measuring kurtosis data.kurtosis(axis=0) . row_id -1.200000 loan_type 2.707997 property_type 29.022540 loan_purpose -1.874865 occupancy 7.561584 loan_amount 9385.071465 preapproval 3.913370 msa_md -1.491417 state_code -1.361784 county_code -1.234460 applicant_ethnicity 2.691639 applicant_race 2.661800 applicant_sex 1.370864 applicant_income 1062.740924 population 18.060147 minority_population_pct 0.179010 ffiecmedian_family_income 1.302638 tract_to_msa_md_income_pct 3.749475 number_of_owner-occupied_units 9.540237 number_of_1_to_4_family_units 12.029859 lender -1.105622 co_applicant -1.833520 accepted -2.000007 dtype: float64 . data.columns . Index([&#39;row_id&#39;, &#39;loan_type&#39;, &#39;property_type&#39;, &#39;loan_purpose&#39;, &#39;occupancy&#39;, &#39;loan_amount&#39;, &#39;preapproval&#39;, &#39;msa_md&#39;, &#39;state_code&#39;, &#39;county_code&#39;, &#39;applicant_ethnicity&#39;, &#39;applicant_race&#39;, &#39;applicant_sex&#39;, &#39;applicant_income&#39;, &#39;population&#39;, &#39;minority_population_pct&#39;, &#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;, &#39;number_of_owner-occupied_units&#39;, &#39;number_of_1_to_4_family_units&#39;, &#39;lender&#39;, &#39;co_applicant&#39;, &#39;accepted&#39;], dtype=&#39;object&#39;) . sns.lmplot(x=&#39;applicant_income&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e1cc8f110&gt; . sns.lmplot(x=&#39;population&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d78decd10&gt; . sns.lmplot(x=&#39;lender&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d9cebc2d0&gt; . sns.lmplot(x=&#39;minority_population_pct&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e1cc28dd0&gt; . sns.lmplot(x=&#39;ffiecmedian_family_income&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e00f7ca10&gt; . sns.lmplot(x=&#39;tract_to_msa_md_income_pct&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e1cf2a210&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8ddddb7510&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;loan_amount&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e2444bb90&gt; . sns.lmplot(x=&#39;applicant_income&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e1cf30f10&gt; . sns.lmplot(x=&#39;minority_population_pct&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8ca7a3d0&gt; . sns.lmplot(x=&#39;ffiecmedian_family_income&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e1cc90ed0&gt; . sns.lmplot(x=&#39;tract_to_msa_md_income_pct&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c5bce90&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8ca4cf10&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;population&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c675290&gt; . sns.lmplot(x=&#39;minority_population_pct&#39;,y=&#39;applicant_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c6da490&gt; . sns.lmplot(x=&#39;ffiecmedian_family_income&#39;,y=&#39;applicant_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c791410&gt; . sns.lmplot(x=&#39;tract_to_msa_md_income_pct&#39;,y=&#39;applicant_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d9ce91990&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;applicant_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d9ce98750&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;applicant_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e0a287c90&gt; . sns.lmplot(x=&#39;ffiecmedian_family_income&#39;,y=&#39;minority_population_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e0a28a710&gt; . sns.lmplot(x=&#39;tract_to_msa_md_income_pct&#39;,y=&#39;minority_population_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c4a8310&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;minority_population_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d790034d0&gt; . sns.lmplot(x=&#39;tract_to_msa_md_income_pct&#39;,y=&#39;ffiecmedian_family_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d79047e90&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;ffiecmedian_family_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8e0a295d90&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;ffiecmedian_family_income&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d8c85d310&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;tract_to_msa_md_income_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8dcd9f5510&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;tract_to_msa_md_income_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8dbd682710&gt; . sns.lmplot(x=&#39;number_of_1_to_4_family_units&#39;,y=&#39;number_of_owner-occupied_units&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d68a169d0&gt; . sns.lmplot(x=&#39;number_of_owner-occupied_units&#39;,y=&#39;minority_population_pct&#39;, data= data, fit_reg=False, hue=&#39;accepted&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f8d893a72d0&gt; . . Feature Engineering . From EDA we identified some of the features are higher skewed. This can affect our classifcation model. . Here we extract some features from the dataset and add to the datasets . #caculating the loan amount per applicant ratio LAPDR data[&#39;LDPR&#39;]= (data[&#39;applicant_income&#39;])/ (data[&#39;loan_amount&#39;]) #caculating the lenders x loantype data[&#39;LLT&#39;]= (data[&#39;lender&#39;])* (data[&#39;loan_type&#39;]) # caculating the property x loan purpose data[&#39;PTLP&#39;]= (data[&#39;property_type&#39;])* (data[&#39;loan_purpose&#39;]) . #caculating the loan amount per applicant ratio LAPDR final[&#39;LDPR&#39;]= (final[&#39;applicant_income&#39;])/ (final[&#39;loan_amount&#39;]) #caculating the lenders x loantype final[&#39;LLT&#39;]= (final[&#39;lender&#39;])* (final[&#39;loan_type&#39;]) # caculating the property x loan purpose final[&#39;PTLP&#39;]= (final[&#39;property_type&#39;])* (final[&#39;loan_purpose&#39;]) . #ie.msd_md acceptance Rate Mean on test data MSDARM= pd.DataFrame(data.groupby([&quot;msa_md&quot;])[&quot;accepted&quot;].mean()) MSDARM.shape . (409, 1) . #train dataset LARM= pd.DataFrame(data.groupby([&#39;lender&#39;])[&#39;accepted&#39;].mean()) data= pd.merge(data,LARM,how=&#39;left&#39;, on=&#39;lender&#39;) . final= pd.merge(final,LARM,how=&#39;left&#39;, on=&#39;lender&#39;) . # Renaming certain columns for better readability data.columns = [&#39;row_id&#39;, &#39;loan_type&#39;, &#39;property_type&#39;, &#39;loan_purpose&#39;, &#39;occupancy&#39;,&#39;loan_amount&#39;, &#39;preapproval&#39;, &#39;msa_md&#39;, &#39;state_code&#39;, &#39;county_code&#39;,&#39;applicant_ethnicity&#39;, &#39;applicant_race&#39;, &#39;applicant_sex&#39;,&#39;applicant_income&#39;, &#39;population&#39;, &#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;, &#39;number_of_owner-occupied_units&#39;, &#39;number_of_1_to_4_family_units&#39;,&#39;lender&#39;, &#39;co_applicant&#39;, &#39;accepted&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;] . # Renaming certain columns for better readability final.columns = [&#39;row_id&#39;, &#39;loan_type&#39;, &#39;property_type&#39;, &#39;loan_purpose&#39;, &#39;occupancy&#39;,&#39;loan_amount&#39;, &#39;preapproval&#39;, &#39;msa_md&#39;, &#39;state_code&#39;, &#39;county_code&#39;,&#39;applicant_ethnicity&#39;, &#39;applicant_race&#39;, &#39;applicant_sex&#39;,&#39;applicant_income&#39;, &#39;population&#39;, &#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;, &#39;number_of_owner-occupied_units&#39;, &#39;number_of_1_to_4_family_units&#39;,&#39;lender&#39;, &#39;co_applicant&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;] . data= pd.merge(data,MSDARM,how=&#39;left&#39;, on=&#39;msa_md&#39;) . final= pd.merge(final,MSDARM,how=&#39;left&#39;, on=&#39;msa_md&#39;) . # Renaming certain columns for better readability data.columns = [&#39;row_id&#39;, &#39;loan_type&#39;, &#39;property_type&#39;, &#39;loan_purpose&#39;, &#39;occupancy&#39;,&#39;loan_amount&#39;, &#39;preapproval&#39;, &#39;msa_md&#39;, &#39;state_code&#39;, &#39;county_code&#39;,&#39;applicant_ethnicity&#39;, &#39;applicant_race&#39;, &#39;applicant_sex&#39;,&#39;applicant_income&#39;, &#39;population&#39;, &#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;, &#39;number_of_owner-occupied_units&#39;, &#39;number_of_1_to_4_family_units&#39;,&#39;lender&#39;, &#39;co_applicant&#39;, &#39;accepted&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;,&#39;MSDARM&#39;] . # Renaming certain columns for better readability final.columns = [&#39;row_id&#39;, &#39;loan_type&#39;, &#39;property_type&#39;, &#39;loan_purpose&#39;, &#39;occupancy&#39;,&#39;loan_amount&#39;, &#39;preapproval&#39;, &#39;msa_md&#39;, &#39;state_code&#39;, &#39;county_code&#39;,&#39;applicant_ethnicity&#39;, &#39;applicant_race&#39;, &#39;applicant_sex&#39;,&#39;applicant_income&#39;, &#39;population&#39;, &#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;, &#39;number_of_owner-occupied_units&#39;, &#39;number_of_1_to_4_family_units&#39;,&#39;lender&#39;, &#39;co_applicant&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;,&#39;MSDARM&#39;] . cat_vars=[&#39;loan_type&#39;,&#39;property_type&#39;,&#39;loan_purpose&#39;,&#39;occupancy&#39;,&#39;preapproval&#39;,&#39;applicant_sex&#39;,&#39;co_applicant&#39;, &#39;applicant_sex&#39;,&#39;applicant_race&#39;,&#39;applicant_ethnicity&#39;,&#39;msa_md&#39;,&#39;state_code&#39;,&#39;county_code&#39;] num_vars=[&#39;loan_amount&#39;,&#39;population&#39;,&#39;applicant_income&#39;,&#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;,&#39;number_of_owner-occupied_units&#39;,&#39;number_of_1_to_4_family_units&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;,&#39;MSDARM&#39;] . # pairwise plot for all variable origanal variable %config InlineBackend.figure_format = &#39;png&#39; g= sns.PairGrid(data=data, hue=&quot;accepted&quot;) g.map(plt.scatter) plt.savefig(&#39;image9.png&#39;)num_vars_=[&#39;loan_amount&#39;,&#39;population&#39;,&#39;applicant_income&#39;,&#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;,&#39;number_of_owner-occupied_units&#39;,&#39;number_of_1_to_4_family_units&#39;, &#39;LDPR&#39;,&#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;,&#39;MSDARM&#39;,&#39;accepted&#39;] # pairwise plot for all variable origanal variable %config InlineBackend.figure_format = &#39;png&#39; sns.pairplot(data[num_vars_]) plt.savefig(&#39;image10.png&#39;)%config InlineBackend.figure_format = &#39;png&#39; fig= plt.figure(figsize=(12,10)) sns.pairplot(data[num_vars], palette=&quot;Set2&quot;,diag_kind=&quot;kde&quot;, height=2).map_upper(sns.kdeplot, cmap=&quot;Blues_d&quot;) plt.savefig(&#39;image11.png&#39;) . Class Balance . Mortgage acceptance rate: 50% . data.accepted.value_counts(1)*100 . 1 50.0228 0 49.9772 Name: accepted, dtype: float64 . is_loan_accepted= data.accepted== 1 loan_accepted= data[is_loan_accepted] loan_is_not_accepted= data.accepted== 0 loan_not_accepted= data[loan_is_not_accepted] print(loan_accepted.shape) print(loan_not_accepted.shape) . (250114, 28) (249886, 28) . sns.countplot(&#39;accepted&#39;, data = data) plt.title(&#39;Distribution of Loan Applicant&#39;) plt.savefig(&#39;image4.png&#39;) . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 500000 entries, 0 to 499999 Data columns (total 28 columns): row_id 500000 non-null int64 loan_type 500000 non-null int64 property_type 500000 non-null int64 loan_purpose 500000 non-null int64 occupancy 500000 non-null int64 loan_amount 500000 non-null float64 preapproval 500000 non-null int64 msa_md 500000 non-null int64 state_code 500000 non-null int64 county_code 500000 non-null int64 applicant_ethnicity 500000 non-null int64 applicant_race 500000 non-null int64 applicant_sex 500000 non-null int64 applicant_income 500000 non-null float64 population 500000 non-null float64 minority_population_pct 500000 non-null float64 ffiecmedian_family_income 500000 non-null float64 tract_to_msa_md_income_pct 500000 non-null float64 number_of_owner-occupied_units 500000 non-null float64 number_of_1_to_4_family_units 500000 non-null float64 lender 500000 non-null int64 co_applicant 500000 non-null bool accepted 500000 non-null int64 LDPR 500000 non-null float64 LLT 500000 non-null int64 PTLP 500000 non-null int64 LARM 500000 non-null float64 MSDARM 500000 non-null float64 dtypes: bool(1), float64(11), int64(16) memory usage: 127.3 MB . final.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 500000 entries, 0 to 499999 Data columns (total 27 columns): row_id 500000 non-null int64 loan_type 500000 non-null int64 property_type 500000 non-null int64 loan_purpose 500000 non-null int64 occupancy 500000 non-null int64 loan_amount 500000 non-null float64 preapproval 500000 non-null int64 msa_md 500000 non-null int64 state_code 500000 non-null int64 county_code 500000 non-null int64 applicant_ethnicity 500000 non-null int64 applicant_race 500000 non-null int64 applicant_sex 500000 non-null int64 applicant_income 500000 non-null float64 population 500000 non-null float64 minority_population_pct 500000 non-null float64 ffiecmedian_family_income 500000 non-null float64 tract_to_msa_md_income_pct 500000 non-null float64 number_of_owner-occupied_units 500000 non-null float64 number_of_1_to_4_family_units 500000 non-null float64 lender 500000 non-null int64 co_applicant 500000 non-null bool LDPR 500000 non-null float64 LLT 500000 non-null int64 PTLP 500000 non-null int64 LARM 499278 non-null float64 MSDARM 500000 non-null float64 dtypes: bool(1), float64(11), int64(15) memory usage: 103.5 MB . . Split Train/Test Set . Let&#39;s split our data into a train and test set. We&#39;ll fit our model with the train set and leave our test set for our last evaluation. . # Create the X and y set X = data.drop(&#39;accepted&#39;, axis=1) y = data.accepted . categorical_features_indices= np.where(X.dtypes != np.float)[0] categorical_features_indices . array([ 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 20, 21, 23, 24]) . # Define train and test X_train, X_test, y_train, y_test= train_test_split(X,y, test_size = 0.3, random_state = 42) print(X_train.shape,y_train.shape) print(X_test.shape,y_test.shape) . (350000, 27) (350000,) (150000, 27) (150000,) . y_train.head() . 226114 1 435187 1 294452 0 427864 0 188822 1 Name: accepted, dtype: int64 . X_train.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant LDPR LLT PTLP LARM MSDARM . 226114 226114 | 1 | 1 | 1 | 1 | 52.0 | 3 | 383 | 25 | 49 | 2 | 5 | 1 | 40.0 | 4101.0 | 6.760 | 66693.0 | 88.103 | 1293.0 | 1647.0 | 2612 | False | 0.769231 | 2612 | 1 | 0.935484 | 0.464900 | . 435187 435187 | 1 | 1 | 3 | 1 | 291.0 | 3 | 358 | 32 | 259 | 2 | 5 | 1 | 195.0 | 3668.0 | 34.298 | 109805.0 | 99.295 | 645.0 | 642.0 | 3873 | True | 0.670103 | 3873 | 3 | 0.803419 | 0.523775 | . 294452 294452 | 1 | 1 | 3 | 1 | 692.0 | 3 | 350 | 38 | 233 | 2 | 5 | 1 | 312.0 | 5201.0 | 9.346 | 85651.0 | 100.000 | 1642.0 | 1766.0 | 5316 | False | 0.450867 | 5316 | 3 | 0.610101 | 0.627844 | . 427864 427864 | 1 | 1 | 2 | 1 | 49.0 | 3 | 205 | 2 | 124 | 2 | 5 | 1 | 40.0 | 2342.0 | 11.994 | 58003.0 | 100.000 | 700.0 | 1366.0 | 878 | True | 0.816327 | 878 | 2 | 0.428756 | 0.462359 | . 188822 188822 | 1 | 1 | 1 | 1 | 212.0 | 2 | 305 | 47 | 68 | 2 | 5 | 1 | 88.0 | 4786.0 | 17.743 | 75595.0 | 100.000 | 1480.0 | 1632.0 | 4791 | True | 0.415094 | 4791 | 1 | 0.806499 | 0.544391 | . x_predict= final x_predict.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant LDPR LLT PTLP LARM MSDARM . 0 0 | 2 | 1 | 3 | 1 | 115.0 | 3 | 101 | 16 | 276 | 2 | 5 | 1 | 74.0 | 6329.0 | 59.536 | 69889.0 | 85.78 | 1874.0 | 2410.0 | 3791 | True | 0.643478 | 7582 | 3 | 0.785178 | 0.495576 | . 1 1 | 1 | 1 | 1 | 1 | 252.0 | 2 | 87 | 20 | 68 | 2 | 5 | 1 | 107.0 | 2473.0 | 8.050 | 65313.0 | 100.00 | 947.0 | 1214.0 | 2839 | True | 0.424603 | 2839 | 1 | 0.371434 | 0.607497 | . 2 2 | 1 | 1 | 1 | 1 | 270.0 | 1 | -1 | -1 | -1 | 2 | 1 | 2 | 119.0 | 4975.0 | 22.901 | 67526.0 | 100.00 | 1327.0 | 1753.0 | 4701 | False | 0.440741 | 4701 | 1 | 0.195064 | 0.338949 | . 3 3 | 2 | 1 | 1 | 1 | 179.0 | 2 | 376 | 20 | 11 | 2 | 2 | 2 | 44.0 | 4795.0 | 29.676 | 57766.0 | 100.00 | 1426.0 | 1765.0 | 2153 | True | 0.245810 | 4306 | 1 | 0.817891 | 0.527134 | . 4 4 | 2 | 1 | 1 | 1 | 36.0 | 2 | 254 | 48 | 156 | 3 | 6 | 3 | 32.0 | 5246.0 | 5.110 | 63332.0 | 100.00 | 1452.0 | 2092.0 | 5710 | False | 0.888889 | 11420 | 1 | 0.489632 | 0.547771 | . . Model Measurement . Metrics used for measurement include following: . Accuracy | AUC | Macro Precision | Macro Recall | F1 Score | . import sklearn.model_selection as ms import sklearn.metrics as sklm . def score_model(probs, threshold): return np.array([1 if x&gt; threshold else 0 for x in probs[:, 1]]) . def print_metrics(labels, probs, threshold): scores = score_model(probs, threshold) metrics = sklm.precision_recall_fscore_support(labels, scores) conf = sklm.confusion_matrix(labels, scores) print(&#39; Confusion Matrix&#39;) print(&#39; Score Positive Score Negative&#39;) print(&#39;Actual Positive %6d&#39; % conf[0,0] + &#39; %5d&#39; % conf[0,1]) print(&#39;Actual Negative %6d&#39; % conf[1,0] + &#39; %5d&#39; % conf[1,1]) print(&#39;&#39;) print(&#39;Accuracy %0.2f&#39; % sklm.accuracy_score(labels, scores)) print(&#39;AUC %0.2f&#39; % sklm.roc_auc_score(labels, probs[:,1])) print(&#39;Macro Precision %0.2f&#39; % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0)) print(&#39;Macro Recall %0.2f&#39; % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0)) print(&#39; &#39;) print(&#39; Positive Negative&#39;) print(&#39;Num Case %6d&#39; % metrics[3][0] + &#39; %6d&#39; % metrics[3][1]) print(&#39;Precision %6.2f&#39; % metrics[0][0] + &#39; %6.2f&#39; % metrics[0][1]) print(&#39;Recall %6.2f&#39; % metrics[1][0] + &#39; %6.2f&#39; % metrics[1][1]) print(&#39;F1 %6.2f&#39; % metrics[2][0] + &#39; %6.2f&#39; % metrics[2][1]) . def plot_auc(labels, probs, threshold): ## compute the false postive rate, true positive rate and threshold along with the AUC pl.style.use(&#39;ggplot&#39;) scores = score_model(probs, threshold) accuracy= sklm.accuracy_score(labels, scores) fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1]) auc = sklm.auc(fpr, tpr) ## plot the result plt.title(&#39;Reciever Operating Charateristic&#39;) plt.plot(fpr, tpr, color = &#39;orange&#39;, label = &#39;AUC = %0.2f&#39; %auc) plt.legend(loc = &#39;lower right&#39;) plt.plot([0,1],[0,1],&#39;r--&#39;) plt.xlim([0,1]) plt.ylim([0,1]) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.title(&quot;Recieve Operating Characteristic (Accuracy= %0.2f)&quot; %accuracy) plt.show() . Train Three Models . Catboost Classifier | Xgboost Classifier | Decision Tree | import catboost as cb from catboost import CatBoostClassifier from xgboost import XGBClassifier from sklearn import tree from sklearn.model_selection import cross_val_score import matplotlib.pylab as pl . . Choose Which Sampling Technique to Use For Model . We all models listed above using a 5-fold cross-validation. . labels= data.accepted features= X fig= plt.figure(figsize=(12,10)) models= [CatBoostClassifier(depth=10,logging_level=&#39;Silent&#39;, random_seed=42), XGBClassifier(n_estimators=10, max_depth=4), tree.DecisionTreeClassifier(random_state= 42)] CV= 5 cv_df= pd.DataFrame(index= range(CV * len(models))) entries= [] for model in models: model_name= model.__class__.__name__ accuracies= cross_val_score(model, features, labels, scoring= &#39;accuracy&#39;, cv= CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df= pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;accuracy&#39;]) sns.boxplot(x= &#39;model_name&#39;, y= &#39;accuracy&#39;, data= cv_df) sns.stripplot(x= &#39;model_name&#39;, y= &#39;accuracy&#39;, data= cv_df, size= 8, jitter= True, edgecolor= &quot;gray&quot;, linewidth= 2) plt.show() print(cv_df.groupby(&#39;model_name&#39;).accuracy.mean()) . model_name CatBoostClassifier 0.726908 DecisionTreeClassifier 0.631850 XGBClassifier 0.708636 Name: accuracy, dtype: float64 . From the results above we can see catboost classifier out performs the other models. We move further to tune the catboost classifier model. . models= [CatBoostClassifier(depth=10,logging_level=&#39;Silent&#39;, random_seed=42), XGBClassifier(n_estimators=10, max_depth=4), tree.DecisionTreeClassifier(random_state= 42)] result_df= pd.DataFrame(columns=[&#39;classifiers&#39;, &#39;fpr&#39;,&#39;tpr&#39;,&#39;auc&#39;]) for model in models: model_name= model.__class__.__name__ model_fit=model.fit(X_train,y_train) y_prob= model_fit.predict_proba(X_test)[::,1] fpr, tpr, threshold = sklm.roc_curve(y_test, y_prob) auc = sklm.auc(fpr, tpr) result_df= result_df.append({&#39;model_name&#39;:model_name,&#39;fpr&#39;:fpr, &#39;tpr&#39;:tpr,&#39;auc&#39;:auc}, ignore_index=True) result_df.set_index(&#39;model_name&#39;, inplace=True) fig= plt.figure(figsize=(+8,6)) pl.style.use(&#39;ggplot&#39;) for i in result_df.index: plt.plot(result_df.loc[i][&#39;fpr&#39;], result_df.loc[i][&#39;tpr&#39;], label=&quot;{}, AUC={:.2f}&quot;.format(i, result_df.loc[i][&#39;auc&#39;])) plt.plot([0,1],[0,1], color=&#39;red&#39;,linestyle=&#39;--&#39;) plt.xticks(np.arange(0.0,1.1,step=0.1)) plt.xlabel(&quot;False Postive Rate&quot;, fontsize=15) plt.yticks(np.arange(0.0,1.1,step=0.1)) plt.ylabel(&quot;True Positive Rate&quot;,fontsize=15) plt.title(&quot;Reciever Operating Charateristic&quot;) plt.legend(prop={&#39;size&#39;:13}, loc=&#39;lower right&#39;) plt.show() . From the results above we can see catboost classifier out performs the other models in terms of AUC We move further to tune the catboost classifier model. With this and the results above we then to tune and improve the catboost classifier model. . Normalizing the features that are highly skewed to make it normally distributed and running the model to see its accuracy and auc . data_norm= data.copy() final_norm= final.copy() . data_norm.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant accepted LDPR LLT PTLP LARM MSDARM . 0 0 | 3 | 1 | 1 | 1 | 70.0 | 3 | 18 | 37 | 246 | 2 | 5 | 1 | 24.0 | 6203.0 | 44.230 | 60588.0 | 50.933 | 716.0 | 2642.0 | 4536 | False | 1 | 0.342857 | 13608 | 1 | 0.837209 | 0.508505 | . 1 1 | 1 | 1 | 3 | 1 | 178.0 | 3 | 369 | 52 | 299 | 1 | 5 | 1 | 57.0 | 5774.0 | 15.905 | 54821.0 | 100.000 | 1622.0 | 2108.0 | 2458 | False | 0 | 0.320225 | 2458 | 3 | 0.168919 | 0.543021 | . 2 2 | 2 | 1 | 3 | 1 | 163.0 | 3 | 16 | 10 | 306 | 2 | 5 | 1 | 67.0 | 6094.0 | 61.270 | 67719.0 | 100.000 | 760.0 | 1048.0 | 5710 | False | 1 | 0.411043 | 11420 | 3 | 0.489632 | 0.508886 | . 3 3 | 1 | 1 | 1 | 1 | 155.0 | 1 | 305 | 47 | 180 | 2 | 5 | 1 | 105.0 | 6667.0 | 6.246 | 78439.0 | 100.000 | 2025.0 | 2299.0 | 5888 | True | 1 | 0.677419 | 5888 | 1 | 0.691964 | 0.544391 | . 4 4 | 1 | 1 | 1 | 1 | 305.0 | 3 | 24 | 37 | 20 | 2 | 3 | 2 | 71.0 | 6732.0 | 100.000 | 63075.0 | 82.200 | 1464.0 | 1847.0 | 289 | False | 1 | 0.232787 | 289 | 1 | 0.542994 | 0.524821 | . final_norm.head() . row_id loan_type property_type loan_purpose occupancy loan_amount preapproval msa_md state_code county_code applicant_ethnicity applicant_race applicant_sex applicant_income population minority_population_pct ffiecmedian_family_income tract_to_msa_md_income_pct number_of_owner-occupied_units number_of_1_to_4_family_units lender co_applicant LDPR LLT PTLP LARM MSDARM . 0 0 | 2 | 1 | 3 | 1 | 115.0 | 3 | 101 | 16 | 276 | 2 | 5 | 1 | 74.0 | 6329.0 | 59.536 | 69889.0 | 85.78 | 1874.0 | 2410.0 | 3791 | True | 0.643478 | 7582 | 3 | 0.785178 | 0.495576 | . 1 1 | 1 | 1 | 1 | 1 | 252.0 | 2 | 87 | 20 | 68 | 2 | 5 | 1 | 107.0 | 2473.0 | 8.050 | 65313.0 | 100.00 | 947.0 | 1214.0 | 2839 | True | 0.424603 | 2839 | 1 | 0.371434 | 0.607497 | . 2 2 | 1 | 1 | 1 | 1 | 270.0 | 1 | -1 | -1 | -1 | 2 | 1 | 2 | 119.0 | 4975.0 | 22.901 | 67526.0 | 100.00 | 1327.0 | 1753.0 | 4701 | False | 0.440741 | 4701 | 1 | 0.195064 | 0.338949 | . 3 3 | 2 | 1 | 1 | 1 | 179.0 | 2 | 376 | 20 | 11 | 2 | 2 | 2 | 44.0 | 4795.0 | 29.676 | 57766.0 | 100.00 | 1426.0 | 1765.0 | 2153 | True | 0.245810 | 4306 | 1 | 0.817891 | 0.527134 | . 4 4 | 2 | 1 | 1 | 1 | 36.0 | 2 | 254 | 48 | 156 | 3 | 6 | 3 | 32.0 | 5246.0 | 5.110 | 63332.0 | 100.00 | 1452.0 | 2092.0 | 5710 | False | 0.888889 | 11420 | 1 | 0.489632 | 0.547771 | . num_vars=[&#39;loan_amount&#39;,&#39;population&#39;,&#39;applicant_income&#39;,&#39;minority_population_pct&#39;,&#39;ffiecmedian_family_income&#39;, &#39;tract_to_msa_md_income_pct&#39;,&#39;number_of_owner-occupied_units&#39;,&#39;number_of_1_to_4_family_units&#39;,&#39;LDPR&#39;, &#39;LLT&#39;,&#39;PTLP&#39;,&#39;LARM&#39;,&#39;MSDARM&#39;] #plotting histogram numerical variables data_norm[num_vars].hist(bins=25, figsize=(30, 20), layout=(7, 3)); . data_norm.skew(axis=0) . row_id 1.286588e-17 loan_type 1.864712e+00 property_type 5.196600e+00 loan_purpose -1.333652e-01 occupancy 2.871840e+00 loan_amount 7.655279e+01 preapproval -2.242003e+00 msa_md 1.353241e-02 state_code -5.974008e-02 county_code 2.309361e-01 applicant_ethnicity 5.802958e-01 applicant_race -1.583676e+00 applicant_sex 1.370674e+00 applicant_income 2.317498e+01 population 2.947782e+00 minority_population_pct 1.068839e+00 ffiecmedian_family_income 8.063549e-01 tract_to_msa_md_income_pct -2.035543e+00 number_of_owner-occupied_units 1.942059e+00 number_of_1_to_4_family_units 2.080321e+00 lender -2.196283e-01 co_applicant 4.080284e-01 accepted -9.120028e-04 LDPR 8.941249e+01 LLT 1.676016e+00 PTLP 6.904792e-01 LARM -9.479196e-02 MSDARM -5.590501e-01 dtype: float64 . # We apply log(x+1) data_norm[&#39;log_loan_amount&#39;]= np.log(data_norm[&#39;loan_amount&#39;]+1) data_norm[&#39;log_LDPR&#39;]= np.log(data_norm[&#39;LDPR&#39;]+1) data_norm[&#39;log_PTLP&#39;]= np.log(data_norm[&#39;PTLP&#39;]+1) data_norm[&#39;log_applicant_income&#39;]= np.log(data_norm[&#39;applicant_income&#39;]+1) data_norm[&#39;log_population&#39;]= np.log(data_norm[&#39;population&#39;]+1) data_norm[&#39;log_minority_population_pct&#39;]= np.log(data_norm[&#39;minority_population_pct&#39;]+1) data_norm[&#39;log_ffiecmedian_family_income&#39;]= np.log(data_norm[&#39;ffiecmedian_family_income&#39;]+1) data_norm[&#39;log_number_of_owner_occupied_units&#39;]= np.log(data_norm[&#39;number_of_owner-occupied_units&#39;]+1) data_norm[&#39;log_number_of_1_to_4_family_units&#39;]= np.log(data_norm[&#39;number_of_1_to_4_family_units&#39;]) data_norm[&#39;pwr_tract_to_msa_md_income_pct&#39;] = np.power(data_norm[&#39;tract_to_msa_md_income_pct&#39;],10) data_norm[&#39;pwr_LLT&#39;]= np.power(data_norm[&#39;LLT&#39;],10) . num_vars_log= [&#39;log_loan_amount&#39;,&#39;log_LDPR&#39;,&#39;log_PTLP&#39;,&#39;log_applicant_income&#39;,&#39;log_population&#39;,&#39;log_minority_population_pct&#39;, &#39;log_ffiecmedian_family_income&#39;,&#39;log_number_of_owner_occupied_units&#39;,&#39;log_number_of_1_to_4_family_units&#39;, &#39;pwr_tract_to_msa_md_income_pct&#39;,&#39;pwr_LLT&#39;,] data_norm[num_vars_log].hist(bins=25, figsize=(30, 20), layout=(6, 2)); . data_norm.skew(axis=0) . row_id 1.286588e-17 loan_type 1.864712e+00 property_type 5.196600e+00 loan_purpose -1.333652e-01 occupancy 2.871840e+00 loan_amount 7.655279e+01 preapproval -2.242003e+00 msa_md 1.353241e-02 state_code -5.974008e-02 county_code 2.309361e-01 applicant_ethnicity 5.802958e-01 applicant_race -1.583676e+00 applicant_sex 1.370674e+00 applicant_income 2.317498e+01 population 2.947782e+00 minority_population_pct 1.068839e+00 ffiecmedian_family_income 8.063549e-01 tract_to_msa_md_income_pct -2.035543e+00 number_of_owner-occupied_units 1.942059e+00 number_of_1_to_4_family_units 2.080321e+00 lender -2.196283e-01 co_applicant 4.080284e-01 accepted -9.120028e-04 LDPR 8.941249e+01 LLT 1.676016e+00 PTLP 6.904792e-01 LARM -9.479196e-02 MSDARM -5.590501e-01 log_loan_amount -1.045326e+00 log_LDPR 3.109443e+00 log_PTLP -8.455082e-02 log_applicant_income 1.644426e-01 log_population -1.581668e-01 log_minority_population_pct -2.711568e-01 log_ffiecmedian_family_income -2.891652e-01 log_number_of_owner_occupied_units -1.100173e+00 log_number_of_1_to_4_family_units -1.615771e+00 pwr_tract_to_msa_md_income_pct -8.940395e-01 pwr_LLT 8.320281e-02 dtype: float64 . # Create the X and y set X_norm = data_norm.drop(&#39;accepted&#39;, axis=1) y_norm = data_norm.accepted . # Define train and test X_train_norm, X_test_norm, y_train_norm, y_test_norm= train_test_split(X_norm,y_norm, test_size = 0.3, random_state = 42) print(X_train_norm.shape,y_train_norm.shape) print(X_test_norm.shape,y_test_norm.shape) . (350000, 38) (350000,) (150000, 38) (150000,) . labels= data_norm.accepted features= X_norm fig= plt.figure(figsize=(12,10)) models= [CatBoostClassifier(depth=10,logging_level=&#39;Silent&#39;, random_seed=42), XGBClassifier(n_estimators=10, max_depth=4), tree.DecisionTreeClassifier(random_state= 42)] CV= 5 cv_df= pd.DataFrame(index= range(CV * len(models))) entries= [] for model in models: model_name= model.__class__.__name__ accuracies= cross_val_score(model, features, labels, scoring= &#39;accuracy&#39;, cv= CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df= pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;accuracy&#39;]) sns.boxplot(x= &#39;model_name&#39;, y= &#39;accuracy&#39;, data= cv_df) sns.stripplot(x= &#39;model_name&#39;, y= &#39;accuracy&#39;, data= cv_df, size= 8, jitter= True, edgecolor= &quot;gray&quot;, linewidth= 2) plt.show() print(cv_df.groupby(&#39;model_name&#39;).accuracy.mean()) . model_name CatBoostClassifier 0.726900 DecisionTreeClassifier 0.632066 XGBClassifier 0.708804 Name: accuracy, dtype: float64 . models= [CatBoostClassifier(depth=10,logging_level=&#39;Silent&#39;, random_seed=42), XGBClassifier(n_estimators=10, max_depth=4), tree.DecisionTreeClassifier(random_state= 42)] result_df= pd.DataFrame(columns=[&#39;classifiers&#39;, &#39;fpr&#39;,&#39;tpr&#39;,&#39;auc&#39;]) for model in models: model_name= model.__class__.__name__ model_fit=model.fit(X_train_norm,y_train_norm) y_prob_norm= model_fit.predict_proba(X_test_norm)[::,1] fpr, tpr, threshold = sklm.roc_curve(y_test_norm, y_prob_norm) auc = sklm.auc(fpr, tpr) result_df= result_df.append({&#39;model_name&#39;:model_name,&#39;fpr&#39;:fpr, &#39;tpr&#39;:tpr,&#39;auc&#39;:auc}, ignore_index=True) result_df.set_index(&#39;model_name&#39;, inplace=True) fig= plt.figure(figsize=(+8,6)) pl.style.use(&#39;ggplot&#39;) for i in result_df.index: plt.plot(result_df.loc[i][&#39;fpr&#39;], result_df.loc[i][&#39;tpr&#39;], label=&quot;{}, AUC={:.2f}&quot;.format(i, result_df.loc[i][&#39;auc&#39;])) plt.plot([0,1],[0,1], color=&#39;red&#39;,linestyle=&#39;--&#39;) plt.xticks(np.arange(0.0,1.1,step=0.1)) plt.xlabel(&quot;False Postive Rate&quot;, fontsize=15) plt.yticks(np.arange(0.0,1.1,step=0.1)) plt.ylabel(&quot;True Positive Rate&quot;,fontsize=15) plt.title(&quot;Reciever Operating Charateristic&quot;) plt.legend(prop={&#39;size&#39;:13}, loc=&#39;lower right&#39;) plt.show() . From Comparing results from Non normalized features and normalized features. We see that the accuracy of non normalized features is slightly higher than normalized features. With results we preceed to using non-normalized features for tunning and feature training. . Tunning of Catboost model . import hyperopt import sys from frozendict import frozendict import shap shap.initjs() . class UAClassifierObjective(object): def __init__(self, dataset, const_params, fold_count): self._dataset = dataset self._const_params = const_params.copy() self._fold_count = fold_count self._evaluated_count = 0 def _to_catboost_params(self, hyper_params): return { &#39;learning_rate&#39;: hyper_params[&#39;learning_rate&#39;], &#39;depth&#39;: hyper_params[&#39;depth&#39;], &#39;l2_leaf_reg&#39;: hyper_params[&#39;l2_leaf_reg&#39;]} # hyperopt optimizes an objective using `__call__` method (e.g. by doing # `foo(hyper_params)`), so we provide one def __call__(self, hyper_params): # join hyper-parameters provided by hyperopt with hyper-parameters # provided by the user params = self._to_catboost_params(hyper_params) params.update(self._const_params) print(&#39;evaluating params={}&#39;.format(params), file=sys.stdout) sys.stdout.flush() # we use cross-validation for objective evaluation, to avoid overfitting scores = cb.cv( pool=self._dataset, params=params, fold_count=self._fold_count, partition_random_seed=42, verbose=False) # scores returns a dictionary with mean and std (per-fold) of metric # value for each cv iteration, we choose minimal value of objective # mean (though it will be better to choose minimal value among all folds) # because noise is additive min_mean_auc = np.min(scores[&#39;test-AUC-mean&#39;]) print(&#39;evaluated score={}&#39;.format(min_mean_auc), file=sys.stdout) self._evaluated_count += 1 print(&#39;evaluated {} times&#39;.format(self._evaluated_count), file=sys.stdout) # negate because hyperopt minimizes the objective return {&#39;loss&#39;: -min_mean_auc, &#39;status&#39;: hyperopt.STATUS_OK} . def find_best_hyper_params(dataset, const_params, max_evals=100): # we are going to optimize these three parameters, though there are a lot more of them (see CatBoost docs) parameter_space = { &#39;learning_rate&#39;: hyperopt.hp.uniform(&#39;learning_rate&#39;, 0.2, 1.0), &#39;depth&#39;: hyperopt.hp.randint(&#39;depth&#39;, 7), &#39;l2_leaf_reg&#39;: hyperopt.hp.uniform(&#39;l2_leaf_reg&#39;, 1, 10)} objective = UAClassifierObjective(dataset=dataset, const_params=const_params, fold_count=6) trials = hyperopt.Trials() best = hyperopt.fmin( fn=objective, space=parameter_space, algo=hyperopt.rand.suggest, max_evals=max_evals, rstate=np.random.RandomState(seed=42)) return best def train_best_model(X, y, const_params, max_evals=100, use_default=False): # convert pandas.DataFrame to catboost.Pool to avoid converting it on each # iteration of hyper-parameters optimization dataset = cb.Pool(X, y, cat_features=categorical_features_indices) if use_default: # pretrained optimal parameters best = { &#39;learning_rate&#39;: 0.4234185321620083, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 9.464266235679002} else: best = find_best_hyper_params(dataset, const_params, max_evals=max_evals) # merge subset of hyper-parameters provided by hyperopt with hyper-parameters # provided by the user hyper_params = best.copy() hyper_params.update(const_params) # drop `use_best_model` because we are going to use entire dataset for # training of the final model hyper_params.pop(&#39;use_best_model&#39;, None) model = cb.CatBoostClassifier(**hyper_params) model.fit(dataset, verbose=False) return model, hyper_params . import time start=time.time() have_gpu = False # skip hyper-parameter optimization and just use provided optimal parameters use_optimal_pretrained_params = False # number of iterations of hyper-parameter search hyperopt_iterations = 50 const_params = frozendict({ &#39;task_type&#39;: &#39;GPU&#39; if have_gpu else &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42}) model, params = train_best_model( X_train, y_train, const_params, max_evals=hyperopt_iterations, use_default=use_optimal_pretrained_params) print(&#39;best params are {}&#39;.format(params), file=sys.stdout) end = time.time() print(end-start) . evaluating params={&#39;learning_rate&#39;: 0.5637608417770977, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 8.493688290834637, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 1 times evaluating params={&#39;learning_rate&#39;: 0.528083167082651, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 7.549531688595925, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310254089012167 evaluated 2 times evaluating params={&#39;learning_rate&#39;: 0.8699106844426274, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 8.949837496427758, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 3 times evaluating params={&#39;learning_rate&#39;: 0.5558660098409214, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 9.268502695024392, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 4 times evaluating params={&#39;learning_rate&#39;: 0.8499167906858907, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 2.546844052569046, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7734748121140296 evaluated 5 times evaluating params={&#39;learning_rate&#39;: 0.8396427532857385, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 4.942262677968309, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 6 times evaluating params={&#39;learning_rate&#39;: 0.7872224143884547, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 9.454327638424944, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 7 times evaluating params={&#39;learning_rate&#39;: 0.693663486801853, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 7.978279409450941, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 8 times evaluating params={&#39;learning_rate&#39;: 0.63472245415225, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 9.280083037935846, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623741421835207 evaluated 9 times evaluating params={&#39;learning_rate&#39;: 0.9643823890479426, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 7.3055930015922925, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.773471890147901 evaluated 10 times evaluating params={&#39;learning_rate&#39;: 0.2029042458037946, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 8.360470176973763, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 11 times evaluating params={&#39;learning_rate&#39;: 0.9033087097584362, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 4.226333703160277, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 12 times evaluating params={&#39;learning_rate&#39;: 0.905182734106353, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 3.7653234749986546, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 13 times evaluating params={&#39;learning_rate&#39;: 0.6216878461972857, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 1.1186473207406844, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7734841160115428 evaluated 14 times evaluating params={&#39;learning_rate&#39;: 0.7096272251393012, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 9.288835555293193, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 15 times evaluating params={&#39;learning_rate&#39;: 0.6666954285134632, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 6.760090014166223, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 16 times evaluating params={&#39;learning_rate&#39;: 0.6126656895443174, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 2.122305933450838, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 17 times evaluating params={&#39;learning_rate&#39;: 0.7536883616570151, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 7.129352121946425, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 18 times evaluating params={&#39;learning_rate&#39;: 0.4305107282251851, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 7.252206709468439, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 19 times evaluating params={&#39;learning_rate&#39;: 0.5182286954655402, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 8.87600708641634, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623741421835207 evaluated 20 times evaluating params={&#39;learning_rate&#39;: 0.2782680588629646, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 7.606937685716839, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310254089012167 evaluated 21 times evaluating params={&#39;learning_rate&#39;: 0.5596670546087271, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 2.0726141256143755, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7734748121140296 evaluated 22 times evaluating params={&#39;learning_rate&#39;: 0.46449369289712517, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 6.339387901117264, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310254089012167 evaluated 23 times evaluating params={&#39;learning_rate&#39;: 0.8314498467716105, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 2.3992161107439527, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7734748121140296 evaluated 24 times evaluating params={&#39;learning_rate&#39;: 0.94205821260823, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 6.774791231932208, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 25 times evaluating params={&#39;learning_rate&#39;: 0.4643443332702141, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 3.7415118170244104, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310275811755531 evaluated 26 times evaluating params={&#39;learning_rate&#39;: 0.8642659638603969, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 7.341252418608859, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 27 times evaluating params={&#39;learning_rate&#39;: 0.7294189282659973, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 1.0626887985889903, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623793574052168 evaluated 28 times evaluating params={&#39;learning_rate&#39;: 0.48343522785711396, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 6.710418002699299, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 29 times evaluating params={&#39;learning_rate&#39;: 0.7210562330475281, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 6.850395695432215, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623741421835207 evaluated 30 times evaluating params={&#39;learning_rate&#39;: 0.5401179224561183, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 2.184936328120133, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7492519266208718 evaluated 31 times evaluating params={&#39;learning_rate&#39;: 0.6308365875061976, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 3.850918241301665, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 32 times evaluating params={&#39;learning_rate&#39;: 0.30502207676284404, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 6.181142547861167, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623741421835207 evaluated 33 times evaluating params={&#39;learning_rate&#39;: 0.6292005093922117, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 6.62131414290977, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 34 times evaluating params={&#39;learning_rate&#39;: 0.4610454641190919, &#39;depth&#39;: 2, &#39;l2_leaf_reg&#39;: 6.43044823439896, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7178748842690442 evaluated 35 times evaluating params={&#39;learning_rate&#39;: 0.6292458327974482, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 2.6565212671416507, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7492519266208718 evaluated 36 times evaluating params={&#39;learning_rate&#39;: 0.5464785404218941, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 5.469164797061436, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 37 times evaluating params={&#39;learning_rate&#39;: 0.8815589660398113, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 7.762603832174364, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310254089012167 evaluated 38 times evaluating params={&#39;learning_rate&#39;: 0.7673527934998763, &#39;depth&#39;: 5, &#39;l2_leaf_reg&#39;: 1.3354940643295945, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7623793574052168 evaluated 39 times evaluating params={&#39;learning_rate&#39;: 0.9509122562293137, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 8.157368633221372, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 40 times evaluating params={&#39;learning_rate&#39;: 0.37343651242935033, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 9.120965626738386, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 41 times evaluating params={&#39;learning_rate&#39;: 0.5825104357671131, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 7.375442989938085, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 42 times evaluating params={&#39;learning_rate&#39;: 0.6370940109439119, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 3.310896149885864, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 43 times evaluating params={&#39;learning_rate&#39;: 0.9703050878380732, &#39;depth&#39;: 3, &#39;l2_leaf_reg&#39;: 4.42644166211115, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.7310254089012167 evaluated 44 times evaluating params={&#39;learning_rate&#39;: 0.6909652662226076, &#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 8.067461444538466, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.773471890147901 evaluated 45 times evaluating params={&#39;learning_rate&#39;: 0.44765747767757863, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 9.72685862525973, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.749250579565882 evaluated 46 times evaluating params={&#39;learning_rate&#39;: 0.4380272549876373, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 3.2977280073078528, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 47 times evaluating params={&#39;learning_rate&#39;: 0.2033304608226356, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 9.813523901250528, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 48 times evaluating params={&#39;learning_rate&#39;: 0.9619808422033007, &#39;depth&#39;: 1, &#39;l2_leaf_reg&#39;: 8.88997567856084, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.6790411481213688 evaluated 49 times evaluating params={&#39;learning_rate&#39;: 0.990250545808629, &#39;depth&#39;: 0, &#39;l2_leaf_reg&#39;: 4.183924523770489, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} evaluated score=0.5 evaluated 50 times 100%|██████████| 50/50 [2:56:50&lt;00:00, 171.83s/it, best loss: -0.7734841160115428] . Warning: Custom metrics will not be evaluated because there are no test datasets . best params are {&#39;depth&#39;: 6, &#39;l2_leaf_reg&#39;: 1.1186473207406844, &#39;learning_rate&#39;: 0.6216878461972857, &#39;task_type&#39;: &#39;CPU&#39;, &#39;loss_function&#39;: &#39;Logloss&#39;, &#39;eval_metric&#39;: &#39;AUC&#39;, &#39;custom_metric&#39;: [&#39;AUC&#39;], &#39;iterations&#39;: 100, &#39;random_seed&#39;: 42} 10644.765050649643 . probabilities = model.predict_proba(data=X_test) print_metrics(y_test, probabilities, 0.51) . Confusion Matrix Score Positive Score Negative Actual Positive 51767 22864 Actual Negative 17783 57586 Accuracy 0.73 AUC 0.81 Macro Precision 0.73 Macro Recall 0.73 Positive Negative Num Case 74631 75369 Precision 0.74 0.72 Recall 0.69 0.76 F1 0.72 0.74 . plot_auc(y_test, probabilities, 0.50) . from catboost import CatBoostClassifier clf_cb= CatBoostClassifier(iterations=2500, depth=10,logging_level=&#39;Silent&#39;, learning_rate=0.01,eval_metric=&#39;Accuracy&#39;,use_best_model=True, random_seed=42) clf_cb.fit(X_test, y_test, cat_features= categorical_features_indices, eval_set=(X_test,y_test)) . &lt;catboost.core.CatBoostClassifier at 0x7f8deae5cf10&gt; . probabilities= clf_cb.predict_proba(data= X_test) print_metrics(y_test, probabilities, 0.51) . Confusion Matrix Score Positive Score Negative Actual Positive 54448 20183 Actual Negative 13663 61706 Accuracy 0.77 AUC 0.86 Macro Precision 0.78 Macro Recall 0.77 Positive Negative Num Case 74631 75369 Precision 0.80 0.75 Recall 0.73 0.82 F1 0.76 0.78 . plot_auc(y_test, probabilities, 0.51) . from catboost import CatBoostClassifier clf_cb= CatBoostClassifier(iterations=2500, depth=6,logging_level=&#39;Silent&#39;, learning_rate=0.3548362548720143,eval_metric=&#39;Accuracy&#39;,l2_leaf_reg=2.683829844728577, use_best_model=True, random_seed=42) clf_cb.fit(X_test, y_test, cat_features= categorical_features_indices, eval_set=(X_test,y_test)) . &lt;catboost.core.CatBoostClassifier at 0x7f8e296d41d0&gt; . probabilities= clf_cb.predict_proba(data= X_test) print_metrics(y_test, probabilities, 0.51) . Confusion Matrix Score Positive Score Negative Actual Positive 58500 16131 Actual Negative 12622 62747 Accuracy 0.81 AUC 0.89 Macro Precision 0.81 Macro Recall 0.81 Positive Negative Num Case 74631 75369 Precision 0.82 0.80 Recall 0.78 0.83 F1 0.80 0.81 . plot_auc(y_test, probabilities, 0.50) . tunning and optimizing catboost algorithm . final_score = clf_cb.predict(data=x_predict) final_score= final_score.astype(np.int) submit= pd.DataFrame({&#39;row_id&#39;:x_predict[&#39;row_id&#39;],&#39;accepted&#39;:final_score}) submit.to_csv(&#39;submission.csv&#39;, index=False) . Catboost Classifier Feature Importance . clf_cb.get_feature_importance(prettified=True) . Feature Index Importances . 0 county_code | 10.394392 | . 1 LARM | 9.530536 | . 2 applicant_income | 6.364388 | . 3 lender | 5.648429 | . 4 state_code | 5.553903 | . 5 LDPR | 5.433531 | . 6 minority_population_pct | 5.136094 | . 7 LLT | 5.083558 | . 8 loan_amount | 5.001768 | . 9 ffiecmedian_family_income | 4.999677 | . 10 population | 4.665573 | . 11 number_of_owner-occupied_units | 4.295008 | . 12 number_of_1_to_4_family_units | 4.156638 | . 13 msa_md | 3.210549 | . 14 MSDARM | 3.206498 | . 15 PTLP | 2.540563 | . 16 tract_to_msa_md_income_pct | 2.470535 | . 17 applicant_race | 2.337708 | . 18 preapproval | 1.836922 | . 19 loan_purpose | 1.757151 | . 20 loan_type | 1.600887 | . 21 applicant_sex | 1.437460 | . 22 applicant_ethnicity | 1.344018 | . 23 occupancy | 1.038224 | . 24 co_applicant | 0.607123 | . 25 property_type | 0.347833 | . 26 row_id | 0.001035 | . shap_values = clf_cb.get_feature_importance(cb.Pool(X, y, cat_features=categorical_features_indices), type=&#39;ShapValues&#39;) . expected_value = shap_values[0,-1] shap_values = shap_values[:,:-1] . shap.summary_plot(shap_values, X) . # across the whole dataset cat_features= data[cat_vars] def plot_shap(cat_cols): for col in cat_cols: shap.dependence_plot(col, shap_values, X) # plotting data for categorical variables plot_shap(cat_features) . # (sum of SHAP value magnitudes over the validation dataset) top_inds = np.argsort(-np.sum(np.abs(shap_values), 0)) # make SHAP plots of the three most important features for i in range(20): shap.dependence_plot(top_inds[i], shap_values, X) . shap_values = clf_cb.get_feature_importance(cb.Pool(X, y, cat_features=categorical_features_indices), type=&#39;ShapValues&#39;) . expected_value = shap_values[0,-1] shap_values = shap_values[:,:-1] . shap.summary_plot(shap_values, X) . Conclusion . Binary Classification: Approved / Denied . Need for application: Help customer and financial institution know if customers are eligible for mortgage approval or not. . Optimizing model: After training the model we sort out to optimize our model. Using Bayesian methods the model improved, optimal parameters were found to be depth=6, l2_leaf_reg=1.119 and learning rate= 0.622. These parameters provided an AUC-ROC of 0.81 and an accuracy of 0.73 at 100 iterations. Increasing the iteration to 2500, we achieved an AUC-ROC of 0.89 and accuracy of 0.81. . In conclusion, we can see that mortgage loan approvals can be done using data from traditional loan application without having key industry features such as credit history, debt to income ratio, etc at an accuracy of 81%. Also, we identified geographical features such as state, country and Metropolitan Statistical Area/ Metropolitan Division codes for the property tract has higher feature importance for our model. Other features that have high importance include lender, applicant income and applicant race. Lastly, a few bits of census information such as the percentage of minorities in the population for that tract and the FFIEC median family income for the MSA/MD which the tract is located had some feature importance to the model. . .",
            "url": "https://emilearthur.github.io/fastblog/2019/08/31/Mortgage_approval_analysis.html",
            "relUrl": "/2019/08/31/Mortgage_approval_analysis.html",
            "date": " • Aug 31, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi 👋, I&#39;m Emile Bondzie-Arthur . A Business and Systems Analyst at Fido MicroCredit Limited in Accra, Ghana. Love Data and working with Data. . 🌱 I’m currently learning PyTorch &amp; Deep Learning &amp; More python . | 👨‍💻 All of my projects are available at https://emilearthur.github.io/ . | . &nbsp; . 📫 How to reach me frederickauthur@hotmail.com .",
          "url": "https://emilearthur.github.io/fastblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://emilearthur.github.io/fastblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}